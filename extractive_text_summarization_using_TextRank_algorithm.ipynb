{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extractive text summarization using TextRank algorithm",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TKqj2jxDWjN",
        "outputId": "92effb88-a83a-427f-f92e-6448533ff8a5"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sSrjtPxCZeq"
      },
      "source": [
        "DOCUMENT = \"\"\"\n",
        "Good afternoon, everyone, and welcome to MIT 6s 191. Introduction to deep learning. My name is Alexandra. Mini. And I'm so excited to be your instructor this year. Along with obviously money, in this new virtual format. 6s. 191 is a two-week boot camp on everything deep learning and will cover a ton of material in only two weeks. So, I think it's really important for us to Dive Right In with these lectures. But before we do that, I do want to motivate exactly why I think this is such.\n",
        "\n",
        "Awesome field to study. And when we taught this class last year, I decided to try introducing the class very differently and instead of me telling the class how great successful 91 is, I wanted to let someone else do that instead. So actually I want to start this year by showing you how we introduce 6s 191 last year.\n",
        "\n",
        " Hi everybody, and welcome to MI6 s-191. Introductory course on. Deep learning part here that a mighty leap one is revolutionising. So many deals from robotics medicine, and everything in between.\n",
        "\n",
        " Fundamentals of this and how you can build some of these incredible algorithms. In fact, this entire speech then video are not real and were created using deep learning and artificial intelligence.\n",
        "\n",
        " And in this class, you'll learn how it has been an honor working today, and I hope you enjoy the course.\n",
        "\n",
        " So in case you can tell that was actually not a real video or audio and the audio, you actually heard was purposely degraded a bit more to even make it more obvious that this was not real, and avoid some potential misuse. Even with the purposely degraded audio, that intro went somewhat viral last year after the chorus, and we got some really great and interesting feedback. And to be honest, after last year, and when we die,\n",
        "\n",
        " Didn't we did this? I thought it was going to be really hard for us to top it this year. But actually I was wrong because the one thing I love about this field is that it's moving. So incredibly fast that even within the past year, the state of the art has significantly Advanced and the video you saw that we used last year use deep learning, but it was not a particularly easy video to create it. Required a full video of Obama speaking and it use this to intelligently stitch together.\n",
        "\n",
        " It's up to the scene to make it look and appear like he was mouthing the words that I said and to see the behind the scenes here. Now, you can see the same video with my voice.\n",
        "\n",
        " Hi everybody, and welcome to MIT 6s. 191. The official introductory course on deep learning taught here at MIT. Now, it's actually possible to use just a single static image, not the full video to achieve the exact same thing. And now you can actually see eight more examples of Obama. Now, just created using just a single static image.\n",
        "\n",
        " More full Dynamic videos, but we can achieve the same incredible realism and result using deep learning. Now, of course.\n",
        "\n",
        " There's nothing restricting us to one person this method generalizes to different faces and there's nothing restricting us even to humans anymore or individuals that the algorithm has ever seen before.\n",
        "\n",
        " Hi everybody, and welcome to MIT 6s. 191. The official introductory course on deep learning taught here at MIT.\n",
        "\n",
        " The ability to generate these types of dynamic and moving videos from only a single image is remarkable to me and it's a testament to the true power of deep learning. In this class. You're going to actually not only learn about the technical basis of this technology. But also, some of the very important and very important ethical and societal implications implications of this work as well.\n",
        "\n",
        " Now, I hope this was a really great way to get you excited about this course and 6s 191. And with that, let's get started. We can actually start by taking a step back and asking ourselves what is deep learning defining, deep learning in the context of intelligence intelligence is actually the ability to process information such that it can be used to inform a future decision. Now, the field of artificial intelligence or AI is a science that actually focuses.\n",
        "\n",
        " And building algorithms to do, exactly this to build algorithms to process information such that they can inform future predictions.\n",
        "\n",
        " Now, machine learning, you can think of this as just a subset of AI that actually focuses on teaching an algorithm to learn from experiences without being explicitly programmed. Now, deep learning takes this idea, even further and to subset of machine learning that focuses on using neural networks to automatically extract useful patterns in raw data, and then using these patterns or features to learn to perform that task. And that's exactly what this class is about.\n",
        "\n",
        " This class is about teaching algorithms how to learn a task directly from raw data. And we want to provide you with a solid foundation, both technically and practically for you to understand under the hood, how these algorithms are built and how they can learn.\n",
        "\n",
        " So this course is split between technical lectures as well as project software Labs will cover the foundation starting today with neural networks, which are really the building blocks of everything that will see in this course. And this year. We also have two brand-new really exciting Hot Topic lectures focusing on uncertainty and probabilistic deep learning as well as algorithmic bias and fairness.\n",
        "\n",
        " Finally, we will conclude with some really exciting guest lectures and student project presentations as part of a final project competition that all of you will be eligible to win. Some really exciting prizes. Now a bit of logistics before we dive into the technical side of the lecture. For those of you taking this course for credit, you will have two options to fulfill your credit requirement. The first option will be to actually work in teams of teams of up to four or in\n",
        "\n",
        " Joy to develop a cool new deep learning idea. Now doing so will make you eligible to win some of the prizes that you can see on the right hand side. And we realize that in the context of this class, which is only two weeks. That's an extremely short amount of time to come up with an impressive project or research idea. So we're not going to be judging you on the novelty of that idea. But rather we're not going to be judging you on the results of that idea, but rather the novelty of the idea you're thinking.\n",
        "\n",
        " Process. And how you how impactful this idea can be but not on the results themselves.\n",
        "\n",
        " On the last day of class. You will actually give a 3 minute presentation to a group of Judges who will then award the winners and the prices. Now, again, three minutes is extremely short to actually present your ideas and present your project. But I do believe that there's an art to presenting and conveying your ideas concisely and clearly in such a short amount of time. So we will be holding you strictly to that to that strict deadline.\n",
        "\n",
        " The second option to fulfill your grade requirement is to write a one-page review on a deep learning paper here. The grade is based more on the clarity of the writing and the technical communication of the main ideas. This will be due on Thursday, the last Thursday at the class and you can pick whatever deep learning paper. You would like, if you would like some pointers. We have provided some guide papers that can help you get started. If you would just like to use one of those for your view, in addition to the final project price.\n",
        "\n",
        " It will also be a rewarding this year. Three lab prices one Associated to each of the software Labs that students will complete again. Completion of the software Labs is not required for grade of this course, but it will make you eligible for some of these cool prices. So please, we encourage everyone to compete for these prizes and get the opportunity to win them all.\n",
        "\n",
        " Please post a Piazza. If you have any questions, visit the course website for announcements and digital recordings of lectures, Etc. And please email us. If you have any questions. Also, there are software labs and office hours, right after each of these technical lectures held in gather town. So please drop by in gather town. To ask any questions about the software Labs specifically on those or more generally about past software Labs, or about the lecture that occurred that day.\n",
        "\n",
        " Now, this team, all this course, has a incredible group of Tas and teaching assistants that you can reach out to, at any time in case, you have any issues or questions about the material that you're learning.\n",
        "\n",
        " And finally, we want to give a huge. Thanks to all of our sponsors, who without their help. This class would not be possible. This is the fourth year that we're teaching this class and each year. It just keeps getting bigger and bigger and bigger. And we really give a huge shout out to our sponsors for helping us make this happen each year.\n",
        "\n",
        " And especially this year in light of the virtual format. So now let's start with the fun stuff. Let's start by asking ourselves a question about why, why do we all care about deep learning and specifically, why do we care right now? To understand that it's important to actually understand first, why is deep learning or house? Deep, learning different from traditional machine learning. Now, traditionally machine learning algorithms, the\n",
        "\n",
        " Find a set of features in their data. Usually, these are features that are handcrafted or hand engineered. And as a result. They tend to be pretty brittle in practice when they're deployed. The key idea of deep learning is to learn these features directly from data in a hierarchical manner. That is. Can we learn if we want to learn how to detect the face, for example, can we learn to first start by detecting edges in the image? Composing these edges together?\n",
        "\n",
        " To detect mid-level features such as a, I or a nose, or a mouth, and then going deeper and composing these features into structural facial features, so that we can recognize this face.\n",
        "\n",
        " This is this hierarchical way of thinking is really core to deep learning as core to everything that we're going to learn in this class.\n",
        "\n",
        " Actually the fundamental building blocks though of deep learning and neural networks have actually existed for decades. So one interesting thing to consider is, why are we studying this now? Now is an incredibly amazing time to study these algorithms. And for one reason is because data has become much more pervasive. These models are extremely hungry for data, and at the moment, we're living in an era where we have more data than ever before.\n",
        "\n",
        " Secondly, these algorithms are massively parallelizable. So they can benefit tremendously from Modern GPU Hardware that simply did not exist when these algorithms were developed. And finally due to open source tool boxes, like tensorflow building and deploying these models has become extremely streamlined.\n",
        "\n",
        " So, let's start actually with the fundamental building block of deep learning and of every neural network that is just a single, neuron also known as a perceptron. So we're going to walk through exactly. What is a perceptron, how it's defined. And we're going to build our way up to deeper neural networks all the way from there. So let's start. We're really at the basic building block.\n",
        "\n",
        " The idea of a perceptron or a single neuron, is actually very simple. So I think it's really important for all of you to understand this at its core. Let's start by actually talking about the forward, propagation of information. Through this single neuron. We can define a set of inputs x. I through XM, which you can see on the left-hand side. And each of these inputs are each of these numbers are multiplied by their corresponding weight. And then\n",
        "\n",
        " Add it together. We take this single number, the result of that Edition, and pass it through. What's called a nonlinear activation function to produce our final output. Why?\n",
        "\n",
        " We can actually, actually, this is not entirely correct, because one thing I forgot to mention is that we also have what's called a bias term in here, which allows you to shift your activation function left or right now, on the right hand side of this diagram, you can actually see this concept Illustrated or written out mathematically as a single equation. You can actually rewrite this in terms of linear algebra, Matrix multiplications and dock products.\n",
        "\n",
        " To represent this a bit more concisely.\n",
        "\n",
        " So let's do that.\n",
        "\n",
        " Let's not do that with x capital x which is a vector of our inputs X1 through XM and capital W, which is a vector of our weights W 1 through 8 WM. So each of these are vectors of length n and the output is very simply obtained by taking their dot product.\n",
        "\n",
        " Adding a bias which in this case is W 0 and then applying a non-linearity g.\n",
        "\n",
        " One thing is that I haven't I've been mentioning it a couple times this non-linearity G. What? Exactly is it? Because I've mentioned it now a couple times. Well, it is a nonlinear function. One common example of this nonlinear activation function is what is known as the sigmoid function. Defined here on the right. In fact, there are many types of nonlinear functions. You can see three more examples here including the sigmoid function.\n",
        "\n",
        " And throughout this presentation, you'll actually see these tensorflow code blocks, which will actually illustrate how we can take some of the topics that were learning in this class and actually, practically use them using the tensorflow software library. Now, the sigmoid activation function, which I presented on the previous slide is very popular since it's a function that gives outputs. It takes us input any real number, any activation value, and it outputs a number.\n",
        "\n",
        " Always between 0 and 1. So this makes it really, really suitable for problems in probability because probabilities also have to be between 0 and 1. So let's make some very well suited for those types of problems in modern deep neural. Networks the relu activation function, which you can see on the right is also extremely popular because of its Simplicity. In this case. It's a piecewise linear function. It is zero before when it's a in the - regime and it is strictly the\n",
        "\n",
        " The function in the positive regime.\n",
        "\n",
        " But one really important question that I hope that you're asking yourselves right now is why do we even need activation functions? And I think actually throughout this course, I do want to say that, no matter what I say in the course. I hope that always your questioning why this is a necessary step. And why do we need each of these steps? Because often these are the questions that can lead to really amazing research breakthroughs. So why do we need activation functions? Now, the point of an activation function,\n",
        "\n",
        " And is to actually introduce nonlinearities into our Network. Because these are nonlinear functions and it allows us to actually deal with nonlinear data. This is extremely important in real life, especially because in the real world data is almost always nonlinear. Imagine. I told you to separate here, the green points from the red points, but all you could use is a single straight line. You might think this is easy with multiple lines, are curved lines, but you cannot\n",
        "\n",
        " only use a single straight line and that's what using a neural network with a linear activation function will be like that makes the problem really hard because no matter how deep the neural network is, you'll only be able to produce a single line decision boundary and you'll only able to separate your space with one line.\n",
        "\n",
        " Now using nonlinear activation functions allows your neural network to approximate arbitrarily complex functions and that's what makes neural networks extraordinarily powerful, let's understand this with a simple example so that we can build up our intuition even further. Imagine I give you this trained Network now with weights on the left hand side 3 and negative 2. This network only has two inputs X1.\n",
        "\n",
        " X2. If we want to get the output of it, we simply do the same story. As I said before. First take a DOT product of our inputs with our weights, add the bias and apply a non-linearity. The let's take a look at what's inside of that non-linearity. It's simply a weighted combination of our inputs and the in the form of a two dimensional line. Because in this case, we only have two inputs.\n",
        "\n",
        " So if we want to compute this output, it's the same stories. Before we take a DOT product of X and W. We add our bias and apply our non-linearity. What about what's inside of this? Non-linearity G. Well, this is just a 2d line.\n",
        "\n",
        " In fact since it's just a two dimensional line, we can even plot it in two dimensional space. This is called the feature space the input space. In this case, the feature space in the input space are equal because we only have one neuron. So in this plot, let me describe what you're seeing. So it on the two axes, you're seeing are two inputs. So on one axis is x. 1, 1 of the inputs, on the other axis is x, 2 are other input, and we can plot the line here, our decision, boundary of this train, a neural network that I\n",
        "\n",
        " If you as a line in this space, now, this line corresponds to actually all of the decisions that this neural network can make because if I give you a new data point, for example here, I'm giving you negative 12. This point lies somewhere in the space specifically at X1 equal to -1 and x 2 equal to 2.\n",
        "\n",
        " That's just a point in the space. I want you to compute its weighted combination and I I can actually follow the perceptron equation to get the answer. So here we can see that if we plug it into the perceptron equation. We get 1 plus -3 -4. And the result would be minus six. We plug that into our nonlinear activation function G and we get a final output of zero point, zero zero two.\n",
        "\n",
        " Now, in fact, remember that the sigmoid function actually divides the space into two parts of the river because it outputs everything between 0 and 1. It's dividing it between a point that point five and a greater than 0.5 and less than 0.5. When the input is less than 0 and greater than 0.5., That's when the input is positive. We can illustrate this space actually, but this feature\n",
        "\n",
        " When we're dealing with a small dimensional data, like in this case, we only have two Dimensions but soon will start to talk about problems where we have thousands or millions or in some cases even billions of input of Weights in our neural network, and then drawing these types of plots becomes extremely challenging and not really possible anymore. But at least, when we're in this regime of small, number of inputs and small number of Weights, we can make these plots to really understand the\n",
        "\n",
        " Entire space and for any new input that we obtain, for example, an input right here. We can see exactly that this point is going to be having an activation function less than 0 and its output will be less than 0.5. The magnitude of that actually is computed by plugging into the perceptron equation. So we can't avoid that. But we can immediately get an answer on the decision boundary depending on which side of this hyperplane that we rely on when we plug it in.\n",
        "\n",
        " So, now that we have an idea of how to build a perceptron. Let's start by building neural networks and seeing how they all come together.\n",
        "\n",
        " So let's revisit that diagram of the perceptron that I showed you before. If there's only a few things that you get from this class. I really want everyone to take away how a perceptron works and there's three steps. Remember them always the dot product. You take a DOT product of your inputs and your weights. You add a bias and you apply your non-linearity. There's three steps.\n",
        "\n",
        " Let's simplify this diagram a little bit. Let's clean up some of the arrows and remove the bias and we can actually see now that every line here has its own Associated weight to it, and I'll remove the bias term. Like I said for Simplicity note that Z here is the result of that dot product plus bias. Before we apply the activation function though, G.\n",
        "\n",
        " The final output though is is simply Y which is equal to the activation function of Z, which is our activation value.\n",
        "\n",
        " Now, if we want to define a multi output neural network, we can simply add another perceptron to this picture. So, instead of having one perceptron now, we have two perceptrons and two outputs. Each one is a normal perceptron exactly. Like we saw before taking its inputs. From each of the X Y X1 through XM. S taking the dot product adding a bias and that's it. Now. We have two outputs each of those perceptrons. Oh, we'll have a different set of\n",
        "\n",
        " Wait, remember that we'll get back to that if we want it. So actually one thing to keep in mind here is because all the inputs are densely connected. Every input has a connection to the weights of every perceptron. These are often called dense layers or sometimes fully connected layers. Now, we're through this class, you're going to get a lot of experienced actually coding up and practically creating some of these algorithms using\n",
        "\n",
        " in a software tool box called tensorflow. So now that we have the understanding of how a single perceptron works and how a dense layer works. This is a stack of perceptrons. Let's try and see how we can actually build up a dense layer like this. All the way from scratch to do that. We can actually start by initializing the two components of our dense layer which are the weights and the biases.\n",
        "\n",
        " Now that we have these two parameters of our neural network of our dense layer. We can actually Define the forward propagation of information. Just like we saw it and learn about already that forward propagation of information is simply the dot product or the matrix. Multiplication of our inputs, with our weights, a Tobias that gives us our activation function here. And then we applied this non-linearity to compute the output.\n",
        "\n",
        " Now tend to flow is actually implemented this dense layer for us. So we don't need to do that from scratch. Instead. We can just call it like shown here. So to create a dense layer with two outputs, we can specify this units equal to 2.\n",
        "\n",
        " Now, let's take a look at what's called a single layered neural network. This is one. We have a single hidden layer between our inputs, and our outputs. This layer is called the hidden layer because unlike an input layer and output layer, the states of this hidden layer are typically unobserved. They're hidden to some extent. They're not strictly enforced either. And since we have this transformation now from the input layer to the hidden layer, and from the hidden layer,\n",
        "\n",
        " The output layer. Each of these layers are going to have their own specified weight. Matrices will call W1 the weight matrices for the first layer and W2 the weight Matrix for the second layer.\n",
        "\n",
        " If we take a zoomed-in look at one of the neurons in this hidden layer, let's take, for example, Z 2. For example, this is the exact same perceptron that we saw before, we can compute its output again, using the exact same story. Taking all of its inputs X1 through XM. Applying a DOT product with the weights, adding a bias and that gives us Z 2. If we look at a different neuron, let's suppose is\n",
        "\n",
        " You three will get a different value here because the weights leading to Z3 or probably different than those leading to Z 2. Now this picture looks a bit messy. So let's try and clean things up a bit more from now on. I'll just use this symbol here to denote what we call this dense layer or fully connected layers. And here, you can actually see an example of how we can create this exact neural network again using\n",
        "\n",
        " Um, to flow with the predefined dense layer notation here. We're creating a sequential model where we can stack layers on top of each other first layer with n neurons. And the second layer with two neurons, the output layer.\n",
        "\n",
        " And if we want to create a deep neural network, all we have to do is keep stacking these layers to create more and more hierarchical models ones where the final output is computed by going deeper and deeper into the network and to implement this in tensorflow. Again, it's very similar as we saw before. Again, using the TF Karis sequential call. We can stack each of these dense layers on top of each other, each one.\n",
        "\n",
        " Defied by the number of neurons in that dense layer and one and two. But with the last output layer fix to two outputs, if that's how many outputs we have.\n",
        "\n",
        " Okay, so that's awesome. Now we have an idea of not only how to build up a neural network directly from a perceptron, but how to compose them together to form a complex deep neural networks. Let's take a look at how we can actually apply them to a very real problem that I believe all of you should care very deeply about.\n",
        "\n",
        " Here's the problem with that. We want to build an AI system to learn to answer, will I pass this class and we can start with a simple to feature model, one feature. Let's say it's the number of lectures that you attend as part of this class. And the second feature is the number of hours that you spend working on your final project. You do have some training data from all of the past participants of successful 91. And we can plot this data on this feature space. Like this, the\n",
        "\n",
        " Points here, actually indicate student. So each point is one student that has passed the class and the red points are students that have failed to pass fail. The class. You can see there where they are. In this feature, space depends on the actual number of hours that they attended the lecture. The number of lectures. They attended and the number of hours they spent on the final project.\n",
        "\n",
        " And then there's you, you spent UF attended for lectures. And you have spent five hours on your final project and you want to understand.\n",
        "\n",
        " Will you or how can you build a neural network? Given everyone else in this class? Will you pass or fail? This class based on the training data that you see. So let's do it. We have now all of the requirements to do this now. So let's build a neural network with two inputs. X1 and X2 with X1 being the number of lectures that we attend x 2 is the number of hours you spend on your final project will have\n",
        "\n",
        " Hidden layer with three units and will feed those into a final probability output by passing this class. And we can see that the probability that we pass is 0.1 or 10%, That's not great. But the reason is because that this model was never actually trained. It's basically just a baby. It's never seen any data. Even though you have seen the data, it hasn't seen any data.\n",
        "\n",
        " Data. And more importantly. You haven't told the model how to interpret this data. It needs to learn about this problem. First. It knows nothing about this class or final projects or any of that.\n",
        "\n",
        " So one of the most important things to do, this is actually you have to tell the model when it's able when it is making bad predictions. In order for it to be able to correct itself. Now the loss of a neural network actually defines exactly this it defines how wrong a prediction was. So it takes as input the predicted outputs and the ground truth outputs. Now, if those two things are very far apart from each other, then the loss will be very large.\n",
        "\n",
        " H on the other hand, the closer, these two things are from each other, the smaller the lost and the more accurate the law. So the model will be. So we always want to minimize the loss. We want to incur the, we want to predict something that's as close as possible to the ground. Truth.\n",
        "\n",
        " Now, let's assume we have not just the data from one student. But as we have, in this case, the data from many students, we now care about, not just how the model did on predicting, just one prediction. But how it did on average across all of these students. This is what we call the empirical loss. And it's simply just the mean, or the average of every loss from each individual example, or each individual student.\n",
        "\n",
        " When training a neural network, we want to find a network that minimizes the empirical loss between our predictions and the true outputs.\n",
        "\n",
        " Now, if we look at the problem of binary classification, where the neural network like we want to do in this case is supposed to answer either yes or no one or zero we can use what is called a softmax cross-entropy loss. Now the softmax cross-entropy loss is actually built is actually written out here and it's defined by actually What's called the cross entropy between two probability distribution.\n",
        "\n",
        " It measures how far apart the ground truth probability distribution is from the predicted probability distribution.\n",
        "\n",
        " Let's suppose instead of predicting binary outputs, will I pass this class or will I not pass this class instead? You want to predict the final grade as a real number not a probability, or as a percentage. We want the the grade that you will get in this class. Now, in this case, because the type of the output is different. We also need to use a different loss here because our outputs are no longer 01, but they\n",
        "\n",
        " Any real number third this, the grade that you're going to get on the final class. So for example here since this is a continuous variable, the grade we want to use What's called the mean squared error. This measures just the squared error. The squared difference between our ground truth and our predictions. Again, averaged over the entire data set.\n",
        "\n",
        " Okay, great. So now we've seen two loss functions. One for classification, binary outputs, as well as regression continuous outputs the problem. Now, I think that we need to start asking ourselves is, how can we take that loss function? We've seen our loss function, we've seen our Network. Now we have to actually understand. How can we put those two things together? How can we use our loss function, to train the weights of our neural networks such that it can actually learn that problem.\n",
        "\n",
        " Well, what we want to do is actually find the weights of the neural network that will minimize the loss of our data set. That essentially means that we want to find the W's in our neural network that minimize J of w. J of w is our empirical cost function that we saw in the previous slides, that average loss over each data point. In the data set.\n",
        "\n",
        " Now remember that W capital W is simply a collection of all of the weights in our neural network, not just from one layer, but from every single layer, so that's w0 from the zeroth layer to the first layer to the second layer all concatenate into one. In this optimization problem. We want to optimize all of the double use to minimize this empirical loss.\n",
        "\n",
        " Now remember our loss function is just a simple function of our weights. If we have only two weights, we can actually plot this entire loss landscape over this grid of weight. So, on the one axis on the bottom, you can see wait number one, and the other one, you can see weight zero. There's only two weights in this neural network, very simple neural network. So we can actually plot for every w0 and W1, what is the loss? What is the error?\n",
        "\n",
        " That we expect to see and obtained from this neural network.\n",
        "\n",
        " Now the whole process of training a neural network, optimizing it is to find the lowest point in this lost landscape that will tell us our optimal. W0 and W1. Now how can we do that? The first thing we have to do is pick a point. So let's pick any w0 W1 starting from this point. We can compute the gradient of the landscape at that point. Now, the gradient tells us the direction.\n",
        "\n",
        " It of highest or steepest Ascent. Okay, so that tells us, which way is up. Okay, if we compute the gradient of our laws, with respect to our weights, that's the derivative our gradient for loss with respect to the weights. That tells us the direction of which way is up on that lost. Landscape from where we stand right now.\n",
        "\n",
        " Instead of going up though. We want to find the lowest loss. So let's take the negative of our gradient and take a small step in that direction.\n",
        "\n",
        " Okay, and this will move us a little bit closer to the lowest point and we just keep repeating this. Now. We compute the gradient at this point and repeat the process until we converge and we will converge to a local minimum. We don't know if it will converge to a global Minima. But at least we know that it should in theory converge to a local minimum.\n",
        "\n",
        " Now, we can summarize this algorithm as follows. This algorithm is also known as gradient descent. So we start by initializing all of our weights randomly and we start, then we Loop until convergence. We start from one of those weights are initial point. We compute the gradient that tells us which way is up. So, we take a step in the opposite direction. We take a small step here, small is computed by, multiplying our gradient by this Factor Ada.\n",
        "\n",
        " And we'll learn more about this Factor later. This factor is called The Learning rate. We'll learn more about that later. Now, again, in tensorflow. We can actually see this pseudocode of gradient descent algorithm written out in code. We can randomize all of our weights that in that basically, initializes our search or optimization process at some point in space. And then we keep looping over and over and over again. We compute the loss. We compute the gradient, and we take a small step of our weights in the direction of that.\n",
        "\n",
        " Radiant, but now let's take a look at this term here. This is the how we actually compute the gradient. This explains how the loss is changing with respect to the weight, but I never actually told you how we compute this. So let's talk about this process, which is actually extremely important, in training, neural networks. It's known as back propagation. So how does backpropagation work? How do we compute this?\n",
        "\n",
        " S gradient. Let's start with a very simple neural network. This is probably the simplest neural network in existence. It only has one input, one hidden neuron, and one output Computing, the gradient of our loss, J of w with respect to one of the weights in this case. Just W2, for example, tells us how much a small change in W 2 is going to affect our loss, J. So, if we move around\n",
        "\n",
        " Day, infinitesimally. Small? How will that affect our loss? That's what the gradient is. Going to tell us of derivative of J of w to. So if we write out this derivative, we can actually apply the chain rule to actually compute it. So what does that look like specifically? We can decompose that derivative into the derivative of J. D Theta, DW.\n",
        "\n",
        " Over d, y, multiplied, by derivative of our output with respect to W2. Now, the question here is with this second part, if we want to compute now, not the derivative of our loss with respect to W2. But now the loss with respect to W1, we can do the same stories before we can apply the chain Rule. Now recursively. So now we have to apply the chain rule again to this second part.\n",
        "\n",
        " Now the second part is expanded even further. So the derivative of our output with respect to Z 1 which is the activation function of this first hidden unit and we can back propagate this information. Now, you can see starting from our loss all the way through W2 and then recursively applying this chain rule again to get to W 1 and this allows us to see both the gradient at both W 2 and W 1. So in this\n",
        "\n",
        " This case just to reiterate. Once again, this is telling us this DJ dw1 is telling us how a small change in our weight is going to affect our loss. So we can see if we increase our weight, a small amount. It will increase our laws. That means we want to decrease the weight to decrease our loss. That's what the gradient tells us. Which direction, we need to step in order to decrease or increase our loss function.\n",
        "\n",
        " Now we showed this here for just two weights, in our neural network because we only have two weights. But imagine we have a very deep neural network. One with more than just two layers of or one layer rather of hidden units. We can just repeat this net. This process of applying recursively applying the chain rule to determine how every single way in the model needs to change to impact that loss. But really all this boils down to just recursively applying this chain rule.\n",
        "\n",
        " Formulation that you can see here.\n",
        "\n",
        " And that's the backpropagation algorithm in theory. It sounds very simple. It's just a very, very basic extension on derivatives and the chain rule, but now let's actually touch on some insights from training. These networks in practice that make this process, much more complicated in practice. And why using back propagation as we saw, there is not always so easy. Now, in practice.\n",
        "\n",
        " Any neural networks and optimization of networks can be extremely difficult and it's actually extremely computationally intensive. Here's the visualization of what a loss landscape of a real neural network can look like visualized on just two dimensions.\n",
        "\n",
        " Now, you can see here that the loss is extremely non-convex. Meaning that it has many, many local minimum that can make using an algorithm like gradient descent. Very very challenging, because gradient descent is always going to step closest to the first local Minima, but it can always get stuck there. So finding how to get to the global Minima or a really good solution for your neural network. Can often be very sensitive to your\n",
        "\n",
        " Parameters such as where the optimizer starts in this lost landscape, if it starts in a potentially bad part of the landscape, it can very easily get stuck in one of these local minimum.\n",
        "\n",
        " Now, recall the equation that we talked about for a gradient descent. This was the equation, a sort of Drew. Your next weight update is going to be your current weights - a small amount called The Learning rate multiplied by the gradient. So we have this minus sign because we want to step in the opposite direction and we multiply it by the gradient or we multiplied by the small number called here called Ada, which is what we call the Learning rate. How\n",
        "\n",
        " Do we want to do the learning now? It's term is actually not just how fast to do the learning. That's maybe not the best way to say it, but it tells us how large should each step. We take in practice, be with regards to that gradient. So, the grading tells us the direction but it doesn't necessarily tell us the magnitude of the direction. So, Ada can tell us, actually, a scale of how much we want to trust that gradient and step in the direction of that gradient in.\n",
        "\n",
        " Is setting even Ada. This one, parameters, one number can be extremely difficult, and I want to give you a quick example of why. So, if you have a very non convex, local lost landscape, where you have local Minima, if you set the learning rate too low, then the model can get stuck in these local Minima. It can never Escape them because it gets it actually does optimize itself, but it optimizes it to a very to a non-optimal.\n",
        "\n",
        " Minimum, and it can converge very slowly as well on the other hand. If we increase our learning rate too much, then we can actually overshoot are Minima and actually diverge and lose control and basically explode the training process completely.\n",
        "\n",
        " One of the challenges is actually how to pray how to use, stable learning rates that are large enough to avoid the local Minima, but small enough so that they don't diverge and convert that they don't diverge completely. So there are small enough to actually converge to that Global spot once they reach it. So, how can we actually set this learning rate? Well, one option which is actually somewhat popular in practice is to\n",
        "\n",
        " Should just try a lot of different learning rates and that actually works. It is a feasible approach. But let's see if we can do something a little bit smarter than that more intelligent. What if we could say instead, how can we build an Adaptive learning rate, that actually looks at its loss landscape and adapts itself to account for what it sees in the landscape. There are actually many types of optimizers that do. Exactly this, this means that the learning rates are no longer fixed. They can\n",
        "\n",
        " an increase or decrease depending on how large the gradient is and that location and how fast we want and how fast we're actually learning and many other options. That could be also with regards to the size of the weights at that point. The magnitudes Etc. In fact, these have been widely explored and published as part of tensorflow as well. And during your Labs, we encourage each of you to really try out each of these different types of optimized.\n",
        "\n",
        " Is an experiment with their performance in different types of problems so that you can gain very important intuition about when to use different types of optimizers are what their advantages are and disadvantages in certain applications as well. So let's try and put all of this together. So here we can see a full loop of using tensorflow, to Define your model, on the first line to find your Optimizer here, you\n",
        "\n",
        " Replace this with any Optimizer that you want here. I'm just using stochastic gradient descent. Like we saw before and feeding it through the model. We Loop forever. We're doing this forward prediction. We predict using our model. We compute the loss. With our prediction. This is exactly the loss is telling us again, how incorrect our prediction is with respect to the ground truth. Why we compute the gradient of our loss with respect to each of the weights in our neural.\n",
        "\n",
        " Work. And finally, we apply those gradients using our Optimizer two-step and update our weights.\n",
        "\n",
        " This is really taking everything that we've learned in the class and the lecture so far. And applying it into one, one, whole piece of code written in tensorflow.\n",
        "\n",
        " So I want to continue this talk and really talk about tips for training these networks in practice. Now that we can focus on this very powerful idea of batching your data into mini batches. So before we saw it with gradient descent that we have the following algorithm this gradient that we saw two computer using back propagation can be actually very intensive to compute especially if it's computed over your entire.\n",
        "\n",
        " Fire training set. So this is a summation over every single data point in the entire dataset. And most real life applications. It is simply not feasible to compute this on every single iteration in your optimization Loop. Alternatively. Let's consider a different variant of this algorithm called stochastic, gradient descent. So instead of computing the gradient over our entire data set. Let's just pick a single point compute, the gradient of that single point with respect.\n",
        "\n",
        " Back to the weights and then update all of our weights based on that gradient. So this has some advantages. This is very easy to compute because only using one data point. Now, it's very fast, but it's also very noisy because it's only from one data point instead. There's a middle ground, instead of computing, this noisy gradient of a single point. Let's get a better estimate of our gradient by using a batch be data points.\n",
        "\n",
        " So now, let's pick a batch of be data points and will compute the gradient estimate estimate simply as the average over this batch. So, since be here is usually not that large on the order of tens or hundreds of samples. This is much much faster to compute than regular gradient descent, and it's also much much more accurate than just purely. Stochastic, gradient descent. That only uses a single example. Now, this increases the\n",
        "\n",
        " The gradient accuracy estimation, which also allows us to converge much more smoothly. It also means that we can trust our gradient more than in stochastic gradient descent, so that we can actually increase our Learning grade a bit more as well.\n",
        "\n",
        " Mini batching also leads to massively parallelizable computation. We can split up the batches on separate workers and separate machines and thus achieve even more parallelization and speed increases on our gpus. Now, the last topic I want to talk about is that of overfitting. This is also known as the problem of generalization and is one of the most fundamental problems in all of machine learning and not just deep learning.\n",
        "\n",
        " Now, overfitting, like I said, is critical to understand, so I really want to make sure that this is a clear Concept, in everyone's mind. Ideally, in machine learning. We want to learn a model that accurately describes our test data, not the training data, even though we're optimizing this model based on the training data. What we really want is for it to perform. Well on the test data. So a set differently, we want to build.\n",
        "\n",
        " Representations that can learn from our training data, but still generalize. Well, to unseen test data. Now assume you want to build a line to describe these points. Underfitting means that the model does simply not have enough capacity to represent these points. So no matter how good we try to fit this model. It simply does not have the capacity to represent this type of data on the far right-hand side. We can see the\n",
        "\n",
        " Extreme Other Extreme. We're here at the model is too, complex has too many parameters and it does not generalize well to new data.\n",
        "\n",
        " In the middle though, we can see what's called an ideal fit. It's not over fitting. It's not under fitting, but it has a medium number of parameters, and it's able to fit in a generalizable way to the output and is able to generalize well, to brand new data when it sees it at test time.\n",
        "\n",
        " Now, to address this problem. Let's talk about regularization. How can we make sure that our models do not end up over fit because neural networks, do have a ton of parameters. How can we enforce some form of regularization to them? Now? What is regularization regularization is a technique that constrains our optimization problems such that we can discourage. These complex models from actually being learned and over fit, right? So again, why do we need it?\n",
        "\n",
        " Need it. So that our model can generalize to this unseen data set and a neural networks. We have many techniques for actually imposing regularization onto the model, one very common technique and very simple to understand is called drop out. This is one of the most popular forms of regularization in deep learning and it's very simple. Let's revisit this picture of a neural network. This is a two, layered neural network, two hidden layers and in, drop out.\n",
        "\n",
        " During training. All we simply do is randomly set some of the activations here, 20 with some probability. So what we can do is let's say we pick our probability to be fifty percent. Or 0.5. We can drop randomly for each of the activations. 50% of those neurons. This is extremely powerful as it lowers. The capacity of our neural network so that they have to learn to perform better on tests sets because sometimes\n",
        "\n",
        " And training sets, it just simply cannot rely on some of those parameters. So, it has to be able to be resilient to that kind of drop out. It also means that they're easier to train because at least on every for Passive iterations, we're training only 50% of the weights and only 50% of the gradient. So that also cuts are gradient computation time down in by a factor of two. So, because now we only have to compute half the number of neurons.\n",
        "\n",
        " Gradients.\n",
        "\n",
        " Now on every iteration we dropped out on the previous iteration 50% of neurons, but on the next iteration, we're going to drop out a different set of 50% of the neurons, a different set of neurons and this gives the network it basically forces. The network to learn how to take different Pathways to get to its answer and it can't rely on anyone Pathway to strongly and overfit to that pathway. This is a way to really force it to generalize to this new data.\n",
        "\n",
        " The second regularization technique that we'll talk about is this notion of early stopping? And again here the idea is very basic. It's it's basically let's stop training. Once we realized that our loss is increasing on held out validation or let's call it a test set.\n",
        "\n",
        " So when we start training, we all know the definition of overfitting is when our model starts to perform worse on the test set. So if we set aside some of this training data to be quote, unquote test data, we can monitor how our network is learning on this data and simply just stopped before it has a chance to overfit. So on the x axis, you can see the number of training iterations and on the y-axis, you can see the loss that we get after training that number of iterations.\n",
        "\n",
        " So, as we continue to train in the beginning, both lines continue to decrease. This is as we'd expect, and this is excellent. Since it means our model is getting stronger. Eventually though. The Network's testing loss plateaus and starts to increase. Note that the training accuracy will always continue to go to go down as long as the network, has the capacity to memorize the data and this pattern continues for the rest of training. So it's important here to actually focus on this point here. This is the\n",
        "\n",
        " Point where we need to stop training and after this point, assuming that our Tessa is a valid representation of the true test set. The accuracy of the model will only get worse so we can stop training here. Take this model and this should be the model that we actually use when we deploy into the real world.\n",
        "\n",
        " Anything any model taken from the left hand side is going to be under fit. It's not going to be utilizing the full capacity of the network. And anything taken from the right hand side is over fit and actually performing worse than it needs to on that. Held out test set.\n",
        "\n",
        " So I'll conclude this lecture by summarizing three key points that we've covered so far. We started about the fundamental building blocks of neural networks. The perceptron we learned about stacking and composing these perceptrons together to form complex, hierarchical neural networks and how to mathematically optimize these models with backpropagation. And finally we address the Practical side of these models. That you'll find useful for the labs today, including\n",
        "\n",
        " Adaptive learning rates batching and regularization. So thank you for attending the first lecture and 6s 191. Thank you very much.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Sp4ZBphDBTy"
      },
      "source": [
        "import re\n",
        "\n",
        "DOCUMENT = re.sub(r'\\n|\\r', ' ', DOCUMENT)\n",
        "DOCUMENT = re.sub(r' +', ' ', DOCUMENT)\n",
        "DOCUMENT = DOCUMENT.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJfaklbaDGm3",
        "outputId": "6a120003-38da-4c9a-a906-fef4ef449c62"
      },
      "source": [
        "from gensim.summarization import summarize\n",
        "\n",
        "print(summarize(DOCUMENT, ratio=0.2, split=False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And when we taught this class last year, I decided to try introducing the class very differently and instead of me telling the class how great successful 91 is, I wanted to let someone else do that instead.\n",
            "So incredibly fast that even within the past year, the state of the art has significantly Advanced and the video you saw that we used last year use deep learning, but it was not a particularly easy video to create it.\n",
            "Now, it's actually possible to use just a single static image, not the full video to achieve the exact same thing.\n",
            "The ability to generate these types of dynamic and moving videos from only a single image is remarkable to me and it's a testament to the true power of deep learning.\n",
            "Now, deep learning takes this idea, even further and to subset of machine learning that focuses on using neural networks to automatically extract useful patterns in raw data, and then using these patterns or features to learn to perform that task.\n",
            "This class is about teaching algorithms how to learn a task directly from raw data.\n",
            "And we want to provide you with a solid foundation, both technically and practically for you to understand under the hood, how these algorithms are built and how they can learn.\n",
            "So this course is split between technical lectures as well as project software Labs will cover the foundation starting today with neural networks, which are really the building blocks of everything that will see in this course.\n",
            "We also have two brand-new really exciting Hot Topic lectures focusing on uncertainty and probabilistic deep learning as well as algorithmic bias and fairness.\n",
            "The first option will be to actually work in teams of teams of up to four or in Joy to develop a cool new deep learning idea.\n",
            "Let's start by asking ourselves a question about why, why do we all care about deep learning and specifically, why do we care right now?\n",
            "To understand that it's important to actually understand first, why is deep learning or house?\n",
            "Now, traditionally machine learning algorithms, the Find a set of features in their data.\n",
            "This is this hierarchical way of thinking is really core to deep learning as core to everything that we're going to learn in this class.\n",
            "Actually the fundamental building blocks though of deep learning and neural networks have actually existed for decades.\n",
            "And finally due to open source tool boxes, like tensorflow building and deploying these models has become extremely streamlined.\n",
            "So, let's start actually with the fundamental building block of deep learning and of every neural network that is just a single, neuron also known as a perceptron.\n",
            "The idea of a perceptron or a single neuron, is actually very simple.\n",
            "Let's start by actually talking about the forward, propagation of information.\n",
            "What's called a nonlinear activation function to produce our final output.\n",
            "We can actually, actually, this is not entirely correct, because one thing I forgot to mention is that we also have what's called a bias term in here, which allows you to shift your activation function left or right now, on the right hand side of this diagram, you can actually see this concept Illustrated or written out mathematically as a single equation.\n",
            "And throughout this presentation, you'll actually see these tensorflow code blocks, which will actually illustrate how we can take some of the topics that were learning in this class and actually, practically use them using the tensorflow software library.\n",
            "So let's make some very well suited for those types of problems in modern deep neural.\n",
            "Networks the relu activation function, which you can see on the right is also extremely popular because of its Simplicity.\n",
            "But one really important question that I hope that you're asking yourselves right now is why do we even need activation functions?\n",
            "Now, the point of an activation function, And is to actually introduce nonlinearities into our Network.\n",
            "Because these are nonlinear functions and it allows us to actually deal with nonlinear data.\n",
            "You might think this is easy with multiple lines, are curved lines, but you cannot only use a single straight line and that's what using a neural network with a linear activation function will be like that makes the problem really hard because no matter how deep the neural network is, you'll only be able to produce a single line decision boundary and you'll only able to separate your space with one line.\n",
            "Now using nonlinear activation functions allows your neural network to approximate arbitrarily complex functions and that's what makes neural networks extraordinarily powerful, let's understand this with a simple example so that we can build up our intuition even further.\n",
            "Imagine I give you this trained Network now with weights on the left hand side 3 and negative 2.\n",
            "First take a DOT product of our inputs with our weights, add the bias and apply a non-linearity.\n",
            "1, 1 of the inputs, on the other axis is x, 2 are other input, and we can plot the line here, our decision, boundary of this train, a neural network that I If you as a line in this space, now, this line corresponds to actually all of the decisions that this neural network can make because if I give you a new data point, for example here, I'm giving you negative 12.\n",
            "I want you to compute its weighted combination and I I can actually follow the perceptron equation to get the answer.\n",
            "We plug that into our nonlinear activation function G and we get a final output of zero point, zero zero two.\n",
            "Now, in fact, remember that the sigmoid function actually divides the space into two parts of the river because it outputs everything between 0 and 1.\n",
            "We can illustrate this space actually, but this feature When we're dealing with a small dimensional data, like in this case, we only have two Dimensions but soon will start to talk about problems where we have thousands or millions or in some cases even billions of input of Weights in our neural network, and then drawing these types of plots becomes extremely challenging and not really possible anymore.\n",
            "But at least, when we're in this regime of small, number of inputs and small number of Weights, we can make these plots to really understand the Entire space and for any new input that we obtain, for example, an input right here.\n",
            "We can see exactly that this point is going to be having an activation function less than 0 and its output will be less than 0.5.\n",
            "The magnitude of that actually is computed by plugging into the perceptron equation.\n",
            "Let's start by building neural networks and seeing how they all come together.\n",
            "Let's clean up some of the arrows and remove the bias and we can actually see now that every line here has its own Associated weight to it, and I'll remove the bias term.\n",
            "Now, if we want to define a multi output neural network, we can simply add another perceptron to this picture.\n",
            "Now, we're through this class, you're going to get a lot of experienced actually coding up and practically creating some of these algorithms using in a software tool box called tensorflow.\n",
            "Let's try and see how we can actually build up a dense layer like this.\n",
            "We can actually start by initializing the two components of our dense layer which are the weights and the biases.\n",
            "Just like we saw it and learn about already that forward propagation of information is simply the dot product or the matrix.\n",
            "Now, let's take a look at what's called a single layered neural network.\n",
            "If we take a zoomed-in look at one of the neurons in this hidden layer, let's take, for example, Z 2.\n",
            "And here, you can actually see an example of how we can create this exact neural network again using Um, to flow with the predefined dense layer notation here.\n",
            "And if we want to create a deep neural network, all we have to do is keep stacking these layers to create more and more hierarchical models ones where the final output is computed by going deeper and deeper into the network and to implement this in tensorflow.\n",
            "Let's take a look at how we can actually apply them to a very real problem that I believe all of you should care very deeply about.\n",
            "We want to build an AI system to learn to answer, will I pass this class and we can start with a simple to feature model, one feature.\n",
            "Let's say it's the number of lectures that you attend as part of this class.\n",
            "In this feature, space depends on the actual number of hours that they attended the lecture.\n",
            "So let's build a neural network with two inputs.\n",
            "X1 and X2 with X1 being the number of lectures that we attend x 2 is the number of hours you spend on your final project will have Hidden layer with three units and will feed those into a final probability output by passing this class.\n",
            "So one of the most important things to do, this is actually you have to tell the model when it's able when it is making bad predictions.\n",
            "Now the loss of a neural network actually defines exactly this it defines how wrong a prediction was.\n",
            "When training a neural network, we want to find a network that minimizes the empirical loss between our predictions and the true outputs.\n",
            "Now, if we look at the problem of binary classification, where the neural network like we want to do in this case is supposed to answer either yes or no one or zero we can use what is called a softmax cross-entropy loss.\n",
            "You want to predict the final grade as a real number not a probability, or as a percentage.\n",
            "We also need to use a different loss here because our outputs are no longer 01, but they Any real number third this, the grade that you're going to get on the final class.\n",
            "So for example here since this is a continuous variable, the grade we want to use What's called the mean squared error.\n",
            "Now, I think that we need to start asking ourselves is, how can we take that loss function?\n",
            "We've seen our loss function, we've seen our Network.\n",
            "How can we use our loss function, to train the weights of our neural networks such that it can actually learn that problem.\n",
            "Well, what we want to do is actually find the weights of the neural network that will minimize the loss of our data set.\n",
            "That essentially means that we want to find the W's in our neural network that minimize J of w.\n",
            "J of w is our empirical cost function that we saw in the previous slides, that average loss over each data point.\n",
            "Now remember that W capital W is simply a collection of all of the weights in our neural network, not just from one layer, but from every single layer, so that's w0 from the zeroth layer to the first layer to the second layer all concatenate into one.\n",
            "We want to optimize all of the double use to minimize this empirical loss.\n",
            "Now the whole process of training a neural network, optimizing it is to find the lowest point in this lost landscape that will tell us our optimal.\n",
            "So let's take the negative of our gradient and take a small step in that direction.\n",
            "Now. We compute the gradient at this point and repeat the process until we converge and we will converge to a local minimum.\n",
            "We take a small step here, small is computed by, multiplying our gradient by this Factor Ada. And we'll learn more about this Factor later.\n",
            "We can actually see this pseudocode of gradient descent algorithm written out in code.\n",
            "We compute the gradient, and we take a small step of our weights in the direction of that.\n",
            "This is the how we actually compute the gradient.\n",
            "This explains how the loss is changing with respect to the weight, but I never actually told you how we compute this.\n",
            "So let's talk about this process, which is actually extremely important, in training, neural networks.\n",
            "Let's start with a very simple neural network.\n",
            "It only has one input, one hidden neuron, and one output Computing, the gradient of our loss, J of w with respect to one of the weights in this case.\n",
            "Just W2, for example, tells us how much a small change in W 2 is going to affect our loss, J.\n",
            "Now, the question here is with this second part, if we want to compute now, not the derivative of our loss with respect to W2.\n",
            "Now, you can see starting from our loss all the way through W2 and then recursively applying this chain rule again to get to W 1 and this allows us to see both the gradient at both W 2 and W 1.\n",
            "That means we want to decrease the weight to decrease our loss.\n",
            "Which direction, we need to step in order to decrease or increase our loss function.\n",
            "But imagine we have a very deep neural network.\n",
            "This process of applying recursively applying the chain rule to determine how every single way in the model needs to change to impact that loss.\n",
            "It's just a very, very basic extension on derivatives and the chain rule, but now let's actually touch on some insights from training.\n",
            "Here's the visualization of what a loss landscape of a real neural network can look like visualized on just two dimensions.\n",
            "Your next weight update is going to be your current weights - a small amount called The Learning rate multiplied by the gradient.\n",
            "So we have this minus sign because we want to step in the opposite direction and we multiply it by the gradient or we multiplied by the small number called here called Ada, which is what we call the Learning rate.\n",
            "So, Ada can tell us, actually, a scale of how much we want to trust that gradient and step in the direction of that gradient in.\n",
            "One of the challenges is actually how to pray how to use, stable learning rates that are large enough to avoid the local Minima, but small enough so that they don't diverge and convert that they don't diverge completely.\n",
            "So, how can we actually set this learning rate?\n",
            "What if we could say instead, how can we build an Adaptive learning rate, that actually looks at its loss landscape and adapts itself to account for what it sees in the landscape.\n",
            "They can an increase or decrease depending on how large the gradient is and that location and how fast we want and how fast we're actually learning and many other options.\n",
            "Why we compute the gradient of our loss with respect to each of the weights in our neural.\n",
            "And finally, we apply those gradients using our Optimizer two-step and update our weights.\n",
            "This is really taking everything that we've learned in the class and the lecture so far.\n",
            "Let's consider a different variant of this algorithm called stochastic, gradient descent.\n",
            "So instead of computing the gradient over our entire data set.\n",
            "Let's just pick a single point compute, the gradient of that single point with respect.\n",
            "There's a middle ground, instead of computing, this noisy gradient of a single point.\n",
            "Let's get a better estimate of our gradient by using a batch be data points.\n",
            "So now, let's pick a batch of be data points and will compute the gradient estimate estimate simply as the average over this batch.\n",
            "It also means that we can trust our gradient more than in stochastic gradient descent, so that we can actually increase our Learning grade a bit more as well.\n",
            "Representations that can learn from our training data, but still generalize.\n",
            "These complex models from actually being learned and over fit, right?\n",
            "So that our model can generalize to this unseen data set and a neural networks.\n",
            "We have many techniques for actually imposing regularization onto the model, one very common technique and very simple to understand is called drop out.\n",
            "Let's revisit this picture of a neural network.\n",
            "The capacity of our neural network so that they have to learn to perform better on tests sets because sometimes And training sets, it just simply cannot rely on some of those parameters.\n",
            "Once we realized that our loss is increasing on held out validation or let's call it a test set.\n",
            "The Network's testing loss plateaus and starts to increase.\n",
            "We started about the fundamental building blocks of neural networks.\n",
            "The perceptron we learned about stacking and composing these perceptrons together to form complex, hierarchical neural networks and how to mathematically optimize these models with backpropagation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H83_YvADI2j",
        "outputId": "cf68507e-f1d4-4f63-d1f1-8027c38efb6d"
      },
      "source": [
        "print(summarize(DOCUMENT, word_count=75, split=False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So, let's start actually with the fundamental building block of deep learning and of every neural network that is just a single, neuron also known as a perceptron.\n",
            "We can illustrate this space actually, but this feature When we're dealing with a small dimensional data, like in this case, we only have two Dimensions but soon will start to talk about problems where we have thousands or millions or in some cases even billions of input of Weights in our neural network, and then drawing these types of plots becomes extremely challenging and not really possible anymore.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4ObNmxqDL1N",
        "outputId": "c1406337-e61e-4aac-d981-e7a72b344ba8"
      },
      "source": [
        "sentences = nltk.sent_tokenize(DOCUMENT)\n",
        "len(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "605"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWTCv0YQDPYR",
        "outputId": "e4d14ed3-1a9f-4095-fa73-c3c17547cefa"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "\n",
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = nltk.word_tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if (token not in stop_words)]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)\n",
        "\n",
        "norm_sentences = normalize_corpus(sentences)\n",
        "norm_sentences[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['good afternoon everyone welcome mit',\n",
              "       'introduction deep learning', 'name alexandra'], dtype='<U265')"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "-aGDxNQyDpiY",
        "outputId": "4d422eff-42d6-4b06-d2c6-22004e628b4c"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
        "dt_matrix = tv.fit_transform(norm_sentences)\n",
        "dt_matrix = dt_matrix.toarray()\n",
        "\n",
        "vocab = tv.get_feature_names()\n",
        "td_matrix = dt_matrix.T\n",
        "print(td_matrix.shape)\n",
        "pd.DataFrame(np.round(td_matrix, 2), index=vocab).head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1162, 605)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>565</th>\n",
              "      <th>566</th>\n",
              "      <th>567</th>\n",
              "      <th>568</th>\n",
              "      <th>569</th>\n",
              "      <th>570</th>\n",
              "      <th>571</th>\n",
              "      <th>572</th>\n",
              "      <th>573</th>\n",
              "      <th>574</th>\n",
              "      <th>575</th>\n",
              "      <th>576</th>\n",
              "      <th>577</th>\n",
              "      <th>578</th>\n",
              "      <th>579</th>\n",
              "      <th>580</th>\n",
              "      <th>581</th>\n",
              "      <th>582</th>\n",
              "      <th>583</th>\n",
              "      <th>584</th>\n",
              "      <th>585</th>\n",
              "      <th>586</th>\n",
              "      <th>587</th>\n",
              "      <th>588</th>\n",
              "      <th>589</th>\n",
              "      <th>590</th>\n",
              "      <th>591</th>\n",
              "      <th>592</th>\n",
              "      <th>593</th>\n",
              "      <th>594</th>\n",
              "      <th>595</th>\n",
              "      <th>596</th>\n",
              "      <th>597</th>\n",
              "      <th>598</th>\n",
              "      <th>599</th>\n",
              "      <th>600</th>\n",
              "      <th>601</th>\n",
              "      <th>602</th>\n",
              "      <th>603</th>\n",
              "      <th>604</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ability</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>able</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>account</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accurate</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accurately</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>achieve</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>across</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>activation</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>activations</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 605 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0    1    2    3    4    5    ...  599  600  601  602  603  604\n",
              "ability      0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "able         0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "account      0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "accuracy     0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "accurate     0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "accurately   0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "achieve      0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "across       0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "activation   0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "activations  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[10 rows x 605 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHcc7OldhVWH",
        "outputId": "92463cda-c47b-49cf-d5f3-a1375a9a9a55"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidv = TfidfVectorizer().fit(norm_sentences)\n",
        "print(tfidv.transform(norm_sentences).toarray())\n",
        "print(tfidv.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "{'good': 421, 'afternoon': 22, 'everyone': 317, 'welcome': 1127, 'mit': 636, 'introduction': 519, 'deep': 219, 'learning': 554, 'name': 656, 'alexandra': 24, 'mini': 627, 'im': 474, 'excited': 325, 'instructor': 509, 'year': 1155, 'along': 31, 'obviously': 691, 'money': 641, 'new': 669, 'virtual': 1111, 'format': 387, 'twoweek': 1075, 'boot': 106, 'camp': 120, 'everything': 319, 'cover': 194, 'ton': 1055, 'material': 602, 'two': 1073, 'weeks': 1123, 'think': 1038, 'really': 830, 'important': 483, 'us': 1092, 'dive': 262, 'right': 869, 'lectures': 557, 'want': 1118, 'motivate': 643, 'exactly': 321, 'awesome': 75, 'field': 362, 'study': 982, 'taught': 1006, 'class': 136, 'last': 541, 'decided': 214, 'try': 1072, 'introducing': 518, 'differently': 252, 'instead': 508, 'telling': 1016, 'great': 429, 'successful': 986, 'wanted': 1119, 'let': 562, 'someone': 928, 'else': 288, 'actually': 11, 'start': 954, 'showing': 900, 'introduce': 517, 'hi': 450, 'everybody': 316, 'mi': 620, 'introductory': 520, 'course': 193, 'part': 726, 'mighty': 624, 'leap': 551, 'one': 698, 'revolutionising': 866, 'many': 600, 'deals': 212, 'robotics': 872, 'medicine': 612, 'fundamentals': 401, 'build': 115, 'incredible': 492, 'algorithms': 28, 'fact': 348, 'entire': 298, 'speech': 942, 'video': 1107, 'real': 826, 'created': 197, 'using': 1097, 'artificial': 56, 'intelligence': 510, 'youll': 1157, 'learn': 552, 'honor': 456, 'working': 1143, 'today': 1052, 'hope': 458, 'enjoy': 296, 'case': 125, 'tell': 1015, 'audio': 68, 'heard': 445, 'purposely': 811, 'degraded': 228, 'bit': 102, 'even': 312, 'make': 596, 'obvious': 690, 'avoid': 72, 'potential': 767, 'misuse': 635, 'intro': 516, 'went': 1129, 'somewhat': 931, 'viral': 1110, 'chorus': 134, 'got': 422, 'interesting': 514, 'feedback': 360, 'honest': 455, 'die': 249, 'didnt': 248, 'thought': 1042, 'going': 420, 'hard': 441, 'top': 1057, 'wrong': 1151, 'thing': 1036, 'love': 587, 'moving': 647, 'incredibly': 493, 'fast': 354, 'within': 1137, 'past': 734, 'state': 958, 'art': 55, 'significantly': 905, 'advanced': 19, 'saw': 876, 'used': 1094, 'use': 1093, 'particularly': 728, 'easy': 282, 'create': 196, 'required': 854, 'full': 395, 'obama': 687, 'speaking': 938, 'intelligently': 512, 'stitch': 965, 'together': 1053, 'scene': 879, 'look': 579, 'appear': 45, 'like': 568, 'mouthing': 645, 'words': 1140, 'said': 874, 'see': 886, 'behind': 92, 'scenes': 880, 'voice': 1115, 'official': 694, 'possible': 765, 'single': 913, 'static': 960, 'image': 475, 'achieve': 6, 'exact': 320, 'eight': 285, 'examples': 323, 'dynamic': 278, 'videos': 1108, 'realism': 827, 'result': 862, 'theres': 1033, 'nothing': 682, 'restricting': 861, 'person': 746, 'method': 619, 'generalizes': 408, 'different': 251, 'faces': 346, 'humans': 463, 'anymore': 41, 'individuals': 497, 'algorithm': 26, 'ever': 314, 'seen': 888, 'ability': 0, 'generate': 410, 'types': 1077, 'remarkable': 845, 'testament': 1025, 'true': 1069, 'power': 769, 'youre': 1158, 'technical': 1010, 'basis': 85, 'technology': 1014, 'also': 33, 'ethical': 311, 'societal': 923, 'implications': 482, 'work': 1141, 'well': 1128, 'way': 1120, 'get': 411, 'lets': 563, 'started': 955, 'taking': 1000, 'step': 962, 'back': 79, 'asking': 60, 'defining': 226, 'context': 175, 'process': 797, 'information': 500, 'inform': 499, 'future': 402, 'decision': 215, 'ai': 23, 'science': 881, 'focuses': 377, 'building': 116, 'predictions': 780, 'machine': 591, 'subset': 985, 'teaching': 1007, 'experiences': 333, 'without': 1138, 'explicitly': 336, 'programmed': 801, 'takes': 999, 'idea': 467, 'neural': 665, 'networks': 664, 'automatically': 69, 'extract': 341, 'useful': 1095, 'patterns': 738, 'raw': 824, 'data': 206, 'features': 358, 'perform': 743, 'task': 1005, 'thats': 1031, 'directly': 258, 'provide': 806, 'solid': 926, 'foundation': 391, 'technically': 1011, 'practically': 772, 'understand': 1083, 'hood': 457, 'built': 117, 'split': 946, 'project': 802, 'software': 925, 'labs': 538, 'starting': 956, 'blocks': 104, 'brandnew': 112, 'exciting': 326, 'hot': 459, 'topic': 1058, 'focusing': 378, 'uncertainty': 1081, 'probabilistic': 791, 'algorithmic': 27, 'bias': 97, 'fairness': 352, 'finally': 365, 'conclude': 170, 'guest': 435, 'student': 980, 'presentations': 783, 'final': 364, 'competition': 152, 'eligible': 287, 'win': 1135, 'prizes': 790, 'logistics': 576, 'side': 902, 'lecture': 556, 'credit': 199, 'options': 714, 'fulfill': 394, 'requirement': 855, 'first': 369, 'option': 713, 'teams': 1009, 'four': 392, 'joy': 526, 'develop': 245, 'cool': 185, 'hand': 438, 'realize': 828, 'extremely': 344, 'short': 897, 'amount': 37, 'time': 1048, 'come': 148, 'impressive': 486, 'research': 857, 'judging': 528, 'novelty': 684, 'rather': 823, 'results': 863, 'thinking': 1039, 'impactful': 479, 'day': 208, 'give': 414, 'minute': 633, 'presentation': 782, 'group': 434, 'judges': 527, 'award': 73, 'winners': 1136, 'prices': 789, 'three': 1044, 'minutes': 634, 'present': 781, 'ideas': 470, 'believe': 93, 'presenting': 785, 'conveying': 184, 'concisely': 169, 'clearly': 140, 'holding': 454, 'strictly': 975, 'strict': 974, 'deadline': 209, 'second': 884, 'grade': 425, 'write': 1148, 'onepage': 699, 'review': 864, 'paper': 721, 'based': 82, 'clarity': 135, 'writing': 1149, 'communication': 150, 'main': 595, 'due': 276, 'thursday': 1046, 'pick': 749, 'whatever': 1131, 'would': 1147, 'pointers': 761, 'provided': 807, 'guide': 436, 'papers': 722, 'help': 447, 'view': 1109, 'addition': 17, 'price': 788, 'rewarding': 867, 'lab': 537, 'associated': 62, 'students': 981, 'complete': 153, 'completion': 155, 'please': 754, 'encourage': 291, 'compete': 151, 'opportunity': 703, 'post': 766, 'piazza': 748, 'questions': 815, 'visit': 1112, 'website': 1121, 'announcements': 38, 'digital': 254, 'recordings': 834, 'etc': 310, 'email': 289, 'office': 693, 'hours': 460, 'held': 446, 'gather': 404, 'town': 1061, 'drop': 274, 'ask': 59, 'specifically': 939, 'generally': 409, 'occurred': 692, 'team': 1008, 'tas': 1004, 'assistants': 61, 'reach': 825, 'issues': 522, 'huge': 462, 'thanks': 1030, 'sponsors': 947, 'fourth': 393, 'keeps': 531, 'getting': 413, 'bigger': 99, 'shout': 898, 'helping': 448, 'happen': 440, 'especially': 306, 'light': 567, 'fun': 397, 'stuff': 984, 'question': 813, 'care': 124, 'house': 461, 'traditional': 1062, 'traditionally': 1063, 'find': 366, 'set': 893, 'usually': 1098, 'handcrafted': 439, 'engineered': 295, 'tend': 1018, 'pretty': 786, 'brittle': 114, 'practice': 773, 'theyre': 1035, 'deployed': 235, 'key': 532, 'hierarchical': 452, 'manner': 599, 'detect': 242, 'face': 345, 'example': 322, 'detecting': 243, 'edges': 283, 'composing': 160, 'midlevel': 622, 'nose': 679, 'mouth': 644, 'deeper': 220, 'structural': 978, 'facial': 347, 'recognize': 833, 'core': 186, 'fundamental': 400, 'though': 1041, 'existed': 328, 'decades': 213, 'consider': 173, 'studying': 983, 'amazing': 36, 'reason': 831, 'become': 89, 'much': 648, 'pervasive': 747, 'models': 638, 'hungry': 465, 'moment': 640, 'living': 573, 'era': 303, 'secondly': 885, 'massively': 601, 'parallelizable': 723, 'benefit': 94, 'tremendously': 1068, 'modern': 639, 'gpu': 423, 'hardware': 442, 'simply': 911, 'exist': 327, 'developed': 246, 'open': 702, 'source': 936, 'tool': 1056, 'boxes': 110, 'tensorflow': 1020, 'deploying': 236, 'streamlined': 973, 'block': 103, 'every': 315, 'network': 663, 'neuron': 666, 'known': 535, 'perceptron': 741, 'walk': 1117, 'defined': 224, 'basic': 83, 'simple': 907, 'talking': 1003, 'forward': 390, 'propagation': 805, 'define': 223, 'inputs': 505, 'xm': 1153, 'lefthand': 559, 'numbers': 686, 'multiplied': 653, 'corresponding': 188, 'weight': 1124, 'add': 15, 'take': 997, 'number': 685, 'edition': 284, 'pass': 730, 'whats': 1132, 'called': 119, 'nonlinear': 674, 'activation': 8, 'function': 398, 'produce': 798, 'output': 716, 'entirely': 299, 'correct': 187, 'forgot': 385, 'mention': 615, 'term': 1021, 'allows': 29, 'shift': 896, 'left': 558, 'diagram': 247, 'concept': 168, 'illustrated': 473, 'written': 1150, 'mathematically': 603, 'equation': 302, 'rewrite': 868, 'terms': 1022, 'linear': 570, 'algebra': 25, 'matrix': 605, 'multiplications': 652, 'dock': 267, 'products': 800, 'represent': 851, 'capital': 123, 'vector': 1105, 'weights': 1126, 'wm': 1139, 'vectors': 1106, 'length': 560, 'obtained': 689, 'dot': 270, 'product': 799, 'adding': 16, 'applying': 49, 'nonlinearity': 676, 'havent': 444, 'ive': 525, 'mentioning': 617, 'couple': 192, 'times': 1049, 'mentioned': 616, 'common': 149, 'sigmoid': 903, 'functions': 399, 'including': 487, 'throughout': 1045, 'code': 144, 'illustrate': 472, 'topics': 1059, 'library': 564, 'presented': 784, 'previous': 787, 'slide': 916, 'popular': 763, 'since': 912, 'gives': 416, 'outputs': 717, 'input': 504, 'value': 1102, 'always': 35, 'makes': 597, 'suitable': 987, 'problems': 796, 'probability': 793, 'probabilities': 792, 'suited': 988, 'relu': 843, 'simplicity': 909, 'piecewise': 752, 'zero': 1159, 'regime': 838, 'positive': 764, 'need': 659, 'say': 877, 'matter': 606, 'questioning': 814, 'necessary': 658, 'steps': 963, 'often': 695, 'lead': 548, 'breakthroughs': 113, 'point': 760, 'nonlinearities': 675, 'deal': 210, 'life': 566, 'world': 1145, 'almost': 30, 'imagine': 476, 'told': 1054, 'separate': 891, 'green': 431, 'points': 762, 'red': 836, 'could': 191, 'straight': 972, 'line': 569, 'might': 623, 'multiple': 650, 'lines': 571, 'curved': 204, 'problem': 795, 'able': 1, 'boundary': 108, 'space': 937, 'approximate': 51, 'arbitrarily': 52, 'complex': 156, 'extraordinarily': 342, 'powerful': 770, 'intuition': 521, 'trained': 1065, 'negative': 661, 'story': 971, 'apply': 48, 'inside': 506, 'weighted': 1125, 'combination': 147, 'form': 386, 'dimensional': 255, 'compute': 163, 'stories': 970, 'plot': 755, 'feature': 357, 'equal': 301, 'describe': 240, 'seeing': 887, 'axes': 76, 'axis': 77, 'train': 1064, 'corresponds': 189, 'decisions': 216, 'giving': 417, 'lies': 565, 'somewhere': 932, 'follow': 379, 'answer': 40, 'plug': 757, 'plus': 759, 'minus': 632, 'six': 914, 'remember': 846, 'divides': 264, 'parts': 729, 'river': 871, 'dividing': 265, 'five': 372, 'greater': 430, 'less': 561, 'dealing': 211, 'small': 919, 'dimensions': 256, 'soon': 933, 'talk': 1001, 'thousands': 1043, 'millions': 625, 'cases': 126, 'billions': 100, 'drawing': 272, 'plots': 756, 'becomes': 90, 'challenging': 130, 'least': 555, 'obtain': 688, 'magnitude': 593, 'computed': 164, 'plugging': 758, 'cant': 121, 'immediately': 477, 'depending': 232, 'hyperplane': 466, 'rely': 844, 'revisit': 865, 'showed': 899, 'things': 1037, 'away': 74, 'works': 1144, 'simplify': 910, 'little': 572, 'clean': 138, 'arrows': 54, 'remove': 847, 'ill': 471, 'note': 681, 'multi': 649, 'another': 39, 'picture': 750, 'perceptrons': 742, 'normal': 678, 'oh': 696, 'wait': 1116, 'keep': 530, 'mind': 626, 'densely': 231, 'connected': 171, 'connection': 172, 'dense': 230, 'layers': 547, 'sometimes': 930, 'fully': 396, 'lot': 586, 'experienced': 332, 'coding': 145, 'creating': 198, 'box': 109, 'understanding': 1084, 'layer': 545, 'stack': 951, 'scratch': 882, 'initializing': 503, 'components': 158, 'biases': 98, 'parameters': 725, 'already': 32, 'multiplication': 651, 'tobias': 1051, 'applied': 47, 'flow': 375, 'implemented': 481, 'dont': 269, 'call': 118, 'shown': 901, 'specify': 941, 'units': 1086, 'layered': 546, 'hidden': 451, 'unlike': 1087, 'states': 959, 'typically': 1078, 'unobserved': 1088, 'extent': 340, 'enforced': 294, 'either': 286, 'transformation': 1067, 'specified': 940, 'matrices': 604, 'zoomedin': 1161, 'neurons': 667, 'suppose': 992, 'leading': 549, 'probably': 794, 'looks': 580, 'messy': 618, 'symbol': 995, 'denote': 229, 'um': 1080, 'predefined': 775, 'notation': 680, 'sequential': 892, 'model': 637, 'stacking': 952, 'ones': 700, 'implement': 480, 'similar': 906, 'tf': 1028, 'karis': 529, 'defied': 222, 'fix': 373, 'okay': 697, 'compose': 159, 'deeply': 221, 'heres': 449, 'system': 996, 'attend': 65, 'spend': 944, 'training': 1066, 'participants': 727, 'indicate': 495, 'passed': 731, 'failed': 351, 'fail': 350, 'depends': 233, 'actual': 10, 'attended': 66, 'spent': 945, 'uf': 1079, 'given': 415, 'requirements': 856, 'feed': 359, 'passing': 732, 'never': 668, 'basically': 84, 'baby': 78, 'hasnt': 443, 'importantly': 484, 'interpret': 515, 'needs': 660, 'knows': 536, 'projects': 803, 'making': 598, 'bad': 81, 'order': 715, 'loss': 584, 'defines': 225, 'prediction': 779, 'predicted': 777, 'ground': 433, 'truth': 1071, 'far': 353, 'apart': 44, 'large': 540, 'closer': 142, 'smaller': 920, 'lost': 585, 'accurate': 4, 'law': 543, 'minimize': 629, 'incur': 494, 'predict': 776, 'something': 929, 'close': 141, 'assume': 63, 'predicting': 778, 'average': 70, 'across': 7, 'empirical': 290, 'mean': 608, 'individual': 496, 'minimizes': 630, 'binary': 101, 'classification': 137, 'supposed': 993, 'yes': 1156, 'softmax': 924, 'crossentropy': 202, 'cross': 201, 'entropy': 300, 'distribution': 261, 'measures': 611, 'percentage': 740, 'type': 1076, 'longer': 578, 'third': 1040, 'continuous': 178, 'variable': 1103, 'squared': 949, 'error': 304, 'difference': 250, 'averaged': 71, 'weve': 1130, 'regression': 839, 'put': 812, 'essentially': 307, 'means': 610, 'ws': 1152, 'cost': 190, 'slides': 917, 'collection': 146, 'zeroth': 1160, 'concatenate': 167, 'optimization': 706, 'optimize': 707, 'double': 271, 'landscape': 539, 'grid': 432, 'bottom': 107, 'expect': 331, 'whole': 1133, 'optimizing': 712, 'lowest': 590, 'optimal': 705, 'gradient': 426, 'tells': 1017, 'direction': 257, 'highest': 453, 'steepest': 961, 'ascent': 57, 'laws': 544, 'respect': 859, 'derivative': 237, 'stand': 953, 'move': 646, 'repeating': 849, 'repeat': 848, 'converge': 180, 'local': 574, 'minimum': 631, 'know': 534, 'global': 418, 'minima': 628, 'theory': 1032, 'summarize': 989, 'follows': 381, 'descent': 239, 'randomly': 820, 'loop': 581, 'convergence': 181, 'initial': 501, 'opposite': 704, 'multiplying': 655, 'factor': 349, 'ada': 12, 'later': 542, 'rate': 821, 'pseudocode': 808, 'randomize': 819, 'initializes': 502, 'search': 883, 'looping': 582, 'radiant': 818, 'explains': 335, 'changing': 133, 'backpropagation': 80, 'simplest': 908, 'existence': 329, 'computing': 166, 'change': 132, 'affect': 21, 'around': 53, 'infinitesimally': 498, 'chain': 128, 'rule': 873, 'decompose': 217, 'theta': 1034, 'dw': 277, 'recursively': 835, 'expanded': 330, 'unit': 1085, 'propagate': 804, 'reiterate': 842, 'dj': 266, 'increase': 489, 'decrease': 218, 'net': 662, 'determine': 244, 'impact': 478, 'boils': 105, 'formulation': 389, 'sounds': 935, 'extension': 339, 'derivatives': 238, 'touch': 1060, 'insights': 507, 'complicated': 157, 'difficult': 253, 'computationally': 162, 'intensive': 513, 'visualization': 1113, 'visualized': 1114, 'nonconvex': 673, 'meaning': 609, 'closest': 143, 'stuck': 979, 'finding': 367, 'solution': 927, 'sensitive': 890, 'optimizer': 709, 'starts': 957, 'potentially': 768, 'easily': 281, 'recall': 832, 'talked': 1002, 'sort': 934, 'drew': 273, 'next': 670, 'update': 1091, 'current': 203, 'sign': 904, 'multiply': 654, 'maybe': 607, 'best': 95, 'regards': 837, 'grading': 428, 'doesnt': 268, 'necessarily': 657, 'scale': 878, 'trust': 1070, 'setting': 895, 'quick': 816, 'non': 672, 'convex': 183, 'low': 588, 'escape': 305, 'gets': 412, 'optimizes': 711, 'nonoptimal': 677, 'slowly': 918, 'overshoot': 720, 'diverge': 263, 'lose': 583, 'control': 179, 'explode': 337, 'completely': 154, 'challenges': 129, 'pray': 774, 'stable': 950, 'rates': 822, 'enough': 297, 'convert': 182, 'spot': 948, 'feasible': 356, 'approach': 50, 'smarter': 921, 'intelligent': 511, 'adaptive': 13, 'adapts': 14, 'account': 2, 'sees': 889, 'optimizers': 710, 'fixed': 374, 'location': 575, 'size': 915, 'magnitudes': 594, 'widely': 1134, 'explored': 338, 'published': 809, 'optimized': 708, 'experiment': 334, 'performance': 744, 'gain': 403, 'advantages': 20, 'disadvantages': 259, 'certain': 127, 'applications': 46, 'replace': 850, 'stochastic': 966, 'feeding': 361, 'forever': 384, 'incorrect': 488, 'gradients': 427, 'twostep': 1074, 'learned': 553, 'piece': 751, 'continue': 176, 'tips': 1050, 'focus': 376, 'batching': 88, 'batches': 87, 'following': 380, 'computer': 165, 'fire': 368, 'summation': 991, 'dataset': 207, 'iteration': 523, 'alternatively': 34, 'variant': 1104, 'noisy': 671, 'middle': 621, 'better': 96, 'estimate': 308, 'batch': 86, 'tens': 1019, 'hundreds': 464, 'samples': 875, 'faster': 355, 'regular': 840, 'purely': 810, 'uses': 1096, 'increases': 490, 'accuracy': 3, 'estimation': 309, 'smoothly': 922, 'leads': 550, 'computation': 161, 'workers': 1142, 'machines': 592, 'thus': 1047, 'parallelization': 724, 'speed': 943, 'gpus': 424, 'overfitting': 719, 'generalization': 406, 'critical': 200, 'sure': 994, 'clear': 139, 'everyones': 318, 'ideally': 469, 'accurately': 5, 'describes': 241, 'test': 1024, 'representations': 853, 'still': 964, 'generalize': 407, 'unseen': 1090, 'underfitting': 1082, 'capacity': 122, 'fit': 370, 'righthand': 870, 'extreme': 343, 'ideal': 468, 'fitting': 371, 'medium': 613, 'generalizable': 405, 'brand': 111, 'address': 18, 'regularization': 841, 'end': 292, 'enforce': 293, 'technique': 1012, 'constrains': 174, 'discourage': 260, 'techniques': 1013, 'imposing': 485, 'onto': 701, 'forms': 388, 'activations': 9, 'fifty': 363, 'percent': 739, 'lowers': 589, 'tests': 1027, 'sets': 894, 'resilient': 858, 'kind': 533, 'easier': 280, 'passive': 733, 'iterations': 524, 'cuts': 205, 'half': 437, 'dropped': 275, 'forces': 383, 'pathways': 736, 'anyone': 42, 'pathway': 735, 'strongly': 977, 'overfit': 718, 'force': 382, 'notion': 683, 'early': 279, 'stopping': 969, 'stop': 967, 'realized': 829, 'increasing': 491, 'validation': 1101, 'definition': 227, 'worse': 1146, 'aside': 58, 'quote': 817, 'unquote': 1089, 'monitor': 642, 'stopped': 968, 'chance': 131, 'yaxis': 1154, 'beginning': 91, 'wed': 1122, 'excellent': 324, 'stronger': 976, 'eventually': 313, 'testing': 1026, 'plateaus': 753, 'go': 419, 'long': 577, 'memorize': 614, 'pattern': 737, 'continues': 177, 'rest': 860, 'assuming': 64, 'tessa': 1023, 'valid': 1100, 'representation': 852, 'deploy': 234, 'anything': 43, 'taken': 998, 'utilizing': 1099, 'performing': 745, 'summarizing': 990, 'covered': 195, 'practical': 771, 'thank': 1029, 'attending': 67}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBVaQ9cRiNth",
        "outputId": "fe186dc5-223d-4cbd-dcf9-64843800479a"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(cosine_similarity(tfidv.transform(norm_sentences)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         1.         0.         ... 0.07894795 0.         0.        ]\n",
            " [0.         0.         1.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.07894795 0.         ... 1.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         1.         0.41492065]\n",
            " [0.         0.         0.         ... 0.         0.41492065 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLmDllaUDxIV"
      },
      "source": [
        "from scipy.sparse.linalg import svds\n",
        "    \n",
        "def low_rank_svd(matrix, singular_count=2):\n",
        "    u, s, vt = svds(matrix, k=singular_count)\n",
        "    return u, s, vt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnkxRcQ8D08f",
        "outputId": "16ece9d0-4b6a-49df-d0ef-a352c3ad2119"
      },
      "source": [
        "num_sentences = 8\n",
        "num_topics = 3\n",
        "\n",
        "u, s, vt = low_rank_svd(td_matrix, singular_count=num_topics)  \n",
        "print(u.shape, s.shape, vt.shape)\n",
        "term_topic_mat, singular_values, topic_document_mat = u, s, vt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1162, 3) (3,) (3, 605)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bo45Qj3D5SD"
      },
      "source": [
        "sv_threshold = 0.5\n",
        "min_sigma_value = max(singular_values) * sv_threshold\n",
        "singular_values[singular_values < min_sigma_value] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ItEFwCXD8qk",
        "outputId": "1f5b6e2e-ae02-4a3f-b412-b9c360ef29ed"
      },
      "source": [
        "salience_scores = np.sqrt(np.dot(np.square(singular_values), \n",
        "                                 np.square(topic_document_mat)))\n",
        "salience_scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.68190319e-02, 1.63166232e-01, 9.52294867e-17, 1.34224759e-02,\n",
              "       2.62129256e-02, 1.91333186e-02, 4.49030350e-17, 1.24820888e-01,\n",
              "       1.13743191e-01, 1.01839002e-01, 7.72903017e-03, 1.40645984e-01,\n",
              "       1.56896508e-01, 2.96794827e-03, 4.21111751e-02, 1.56227624e-01,\n",
              "       2.66096321e-02, 7.30065218e-02, 1.42538226e-01, 1.07324845e-01,\n",
              "       1.15854621e-01, 4.70806032e-02, 2.57271740e-02, 1.02079135e-01,\n",
              "       1.28564807e-01, 1.13945142e-01, 3.76243745e-02, 6.26016475e-02,\n",
              "       6.38225202e-02, 1.06139198e-02, 2.98025978e-17, 1.36585114e-01,\n",
              "       1.13763353e-01, 1.20450030e-01, 6.77770154e-02, 1.10858493e-01,\n",
              "       4.38737669e-02, 9.86090604e-02, 1.06139198e-02, 2.05742486e-17,\n",
              "       1.36585114e-01, 9.46880722e-02, 1.77832708e-01, 1.25147972e-01,\n",
              "       7.94439252e-02, 9.73102660e-02, 1.82225783e-01, 2.03449757e-01,\n",
              "       7.87168115e-02, 9.31313195e-02, 1.47390720e-01, 2.80061999e-01,\n",
              "       1.35302465e-01, 2.92461017e-01, 9.95343888e-02, 1.78308912e-01,\n",
              "       3.94734727e-02, 1.42357858e-01, 5.64300703e-02, 4.16875591e-02,\n",
              "       2.78148881e-02, 9.59099616e-02, 1.27225999e-01, 7.62756414e-02,\n",
              "       1.36459677e-01, 6.13018623e-02, 4.47873057e-02, 4.83380740e-02,\n",
              "       3.91070164e-02, 2.80423442e-02, 1.03551716e-01, 6.10640903e-02,\n",
              "       7.14560265e-02, 1.99869504e-02, 7.98482564e-04, 9.34436589e-02,\n",
              "       3.69254582e-02, 1.23516633e-01, 3.91478543e-02, 3.21114990e-02,\n",
              "       9.73103524e-02, 5.61092452e-02, 6.93806159e-02, 4.69385271e-02,\n",
              "       3.24445123e-02, 4.56045504e-03, 2.10784931e-02, 1.05709208e-01,\n",
              "       5.51103422e-03, 5.81746845e-02, 1.31690515e-02, 3.09452230e-02,\n",
              "       7.30776964e-02, 8.30160514e-02, 4.61156314e-03, 9.72946000e-02,\n",
              "       9.40768781e-02, 1.71391613e-03, 7.48027206e-02, 2.16681094e-02,\n",
              "       1.73095119e-01, 1.80464356e-01, 1.95376026e-01, 2.11317199e-01,\n",
              "       3.85519159e-01, 2.59409437e-02, 8.65645220e-03, 1.59411133e-02,\n",
              "       3.04093161e-01, 7.00339173e-19, 1.39323342e-01, 3.66539779e-02,\n",
              "       3.93367320e-02, 1.87380050e-01, 2.84616922e-01, 7.38290664e-02,\n",
              "       2.58824250e-02, 2.36291911e-01, 2.93328994e-01, 2.33856645e-02,\n",
              "       2.89729379e-02, 5.48762090e-02, 4.19383631e-01, 5.07726952e-02,\n",
              "       6.42761897e-02, 1.95130801e-01, 3.06656498e-01, 6.13897232e-02,\n",
              "       2.05168868e-01, 9.32315495e-02, 2.01085808e-01, 8.87203344e-02,\n",
              "       1.41123834e-01, 9.06327082e-02, 5.99183929e-02, 1.10827081e-01,\n",
              "       1.33093819e-01, 1.44528844e-17, 2.25800666e-01, 5.31972880e-02,\n",
              "       1.76345564e-02, 3.24282356e-01, 1.47895446e-01, 7.83247557e-02,\n",
              "       8.12817762e-02, 3.96215925e-02, 3.31515767e-03, 1.51086451e-01,\n",
              "       1.55759509e-01, 3.07014696e-02, 7.27089032e-02, 8.58047370e-02,\n",
              "       2.34519251e-01, 1.03822100e-01, 1.35449710e-01, 3.93151584e-02,\n",
              "       7.33513538e-02, 2.49161838e-01, 1.49998227e-01, 5.40669202e-02,\n",
              "       4.82369576e-02, 3.88243212e-02, 1.47709146e-01, 1.28412917e-01,\n",
              "       5.84705392e-02, 2.79626163e-02, 2.48919934e-02, 6.93196899e-02,\n",
              "       2.47735937e-01, 2.32936409e-01, 1.88073034e-01, 4.07094227e-02,\n",
              "       6.80096014e-02, 2.79817939e-01, 3.26040532e-01, 1.71090157e-01,\n",
              "       5.53940061e-01, 5.63665016e-17, 1.45861885e-01, 5.47017653e-03,\n",
              "       2.34626267e-01, 1.95751352e-01, 2.10197098e-01, 3.48232178e-01,\n",
              "       2.76863087e-01, 1.43097120e-01, 3.60178650e-02, 1.07594908e-01,\n",
              "       2.12688074e-01, 8.86671768e-02, 1.38428383e-01, 4.03586162e-02,\n",
              "       3.20168624e-01, 1.30355012e-01, 3.29750466e-01, 5.92277763e-02,\n",
              "       1.69389024e-01, 2.28090912e-01, 9.68356604e-02, 5.21223031e-02,\n",
              "       1.15026105e-02, 1.59234250e-01, 1.87309400e-01, 9.70102566e-02,\n",
              "       6.98781858e-02, 2.86402671e-01, 1.74929250e-01, 1.55691640e-01,\n",
              "       1.09464105e-01, 5.21611681e-03, 4.78339753e-02, 1.54825730e-01,\n",
              "       3.15778237e-01, 1.76253539e-01, 1.28000688e-01, 1.21032316e-01,\n",
              "       6.78329052e-02, 2.61220360e-01, 7.56615259e-02, 3.16773973e-02,\n",
              "       1.17381694e-01, 1.46359191e-01, 6.11659118e-02, 1.26099458e-01,\n",
              "       2.68483328e-01, 3.57799956e-01, 1.30153692e-01, 1.37556933e-01,\n",
              "       1.87188952e-02, 1.10134417e-01, 7.46758488e-18, 2.79530007e-01,\n",
              "       2.44295558e-01, 1.58034275e-01, 1.36652688e-01, 9.34720286e-02,\n",
              "       1.53900361e-01, 1.64916184e-01, 4.62502785e-02, 3.77107077e-01,\n",
              "       6.38238225e-02, 3.39365368e-01, 5.94348736e-01, 1.10482293e-01,\n",
              "       8.73389163e-02, 2.00414842e-01, 2.25983990e-01, 1.89211349e-01,\n",
              "       2.59514653e-02, 6.60722150e-02, 3.94782431e-02, 2.92388477e-01,\n",
              "       4.21111866e-01, 2.04708505e-01, 3.32108785e-01, 2.69196785e-01,\n",
              "       5.54558340e-02, 7.51160309e-03, 2.77253420e-01, 6.45815361e-02,\n",
              "       1.46678205e-01, 3.13652719e-01, 1.74062781e-01, 1.25763471e-01,\n",
              "       1.82296334e-01, 1.33880173e-01, 2.77645319e-02, 1.46994123e-01,\n",
              "       1.41199491e-01, 3.15724604e-01, 1.29030967e-01, 3.62246228e-01,\n",
              "       2.51312668e-01, 2.30059350e-02, 3.84286714e-02, 1.38799655e-01,\n",
              "       3.39658246e-01, 2.63991052e-01, 9.80960689e-02, 3.87062821e-01,\n",
              "       2.01173723e-01, 3.56665718e-02, 2.54279300e-01, 1.60484162e-01,\n",
              "       6.75944032e-02, 2.87187058e-01, 2.85068183e-01, 1.23099612e-01,\n",
              "       1.38905781e-01, 1.77832708e-01, 1.21600742e-01, 5.84538467e-02,\n",
              "       6.18790942e-02, 5.45533248e-02, 2.81985321e-02, 9.80999797e-02,\n",
              "       5.08323602e-01, 7.57010857e-02, 2.20512511e-02, 4.52060920e-01,\n",
              "       3.24282356e-01, 7.84399392e-17, 7.13171755e-01, 1.40367784e-01,\n",
              "       1.04792817e-01, 1.61733859e-01, 1.33594788e-02, 3.15700544e-01,\n",
              "       3.47015995e-01, 6.12168357e-01, 8.11262216e-17, 2.78046677e-01,\n",
              "       8.48875838e-02, 3.75381864e-02, 7.37372314e-02, 1.77113213e-01,\n",
              "       1.77110002e-02, 2.39648002e-01, 8.85614420e-02, 1.84059506e-01,\n",
              "       1.15975514e-01, 1.91621727e-01, 2.02824205e-01, 1.18501015e-01,\n",
              "       1.09062180e-02, 3.33651776e-01, 2.81167208e-01, 2.00210244e-02,\n",
              "       1.40183212e-01, 9.55711498e-02, 3.56098345e-01, 2.71080517e-01,\n",
              "       2.25229947e-01, 2.57843887e-02, 1.57855142e-01, 1.21272603e-01,\n",
              "       2.23086317e-01, 7.56036009e-02, 2.00499871e-01, 9.60003753e-02,\n",
              "       4.40455215e-03, 1.85116262e-02, 3.88445756e-01, 4.27738927e-02,\n",
              "       2.04003348e-01, 1.33262900e-01, 2.09031855e-01, 1.58047392e-01,\n",
              "       1.83934303e-01, 2.06133287e-01, 3.54693652e-01, 5.40001095e-01,\n",
              "       2.88736991e-01, 6.01335065e-01, 3.83848832e-01, 3.78883803e-02,\n",
              "       1.52505519e-01, 2.36617060e-01, 2.94781865e-01, 2.05603241e-01,\n",
              "       6.14544178e-01, 2.47485376e-01, 3.28829801e-03, 3.11994869e-01,\n",
              "       2.26866575e-01, 2.22059269e-17, 5.98727526e-17, 1.08696848e-01,\n",
              "       1.72817365e-01, 5.55926530e-01, 5.17700150e-01, 6.28766003e-17,\n",
              "       2.56681166e-01, 5.85349662e-01, 2.79400415e-01, 4.81876443e-02,\n",
              "       9.51538542e-02, 1.95728626e-01, 3.59218346e-01, 1.16033928e-01,\n",
              "       5.46539175e-17, 3.21554139e-01, 3.31360900e-02, 4.98562436e-02,\n",
              "       3.32914963e-02, 3.28647920e-01, 1.33400364e-01, 2.33321492e-01,\n",
              "       6.33015638e-01, 1.85014027e-01, 2.78771233e-01, 1.46394897e-01,\n",
              "       1.40313638e-01, 1.65896774e-01, 2.95325441e-02, 2.96952213e-01,\n",
              "       1.17771469e-01, 8.30657971e-03, 5.01280927e-01, 5.91167503e-01,\n",
              "       1.81378362e-01, 7.09360503e-01, 2.94495412e-01, 3.57903311e-01,\n",
              "       5.17425223e-02, 7.36789861e-03, 4.46372544e-01, 5.83878578e-01,\n",
              "       5.53974585e-01, 2.57978096e-01, 3.44252024e-01, 2.86680255e-01,\n",
              "       5.68470807e-03, 1.12871973e-01, 1.67428771e-01, 4.71127740e-01,\n",
              "       1.79520915e-01, 2.94043621e-01, 7.58876455e-02, 6.50238055e-02,\n",
              "       5.58151381e-03, 1.49002771e-01, 3.29730296e-01, 1.97638405e-01,\n",
              "       1.99738282e-02, 7.67418784e-02, 4.00220770e-02, 1.53295972e-01,\n",
              "       3.29655643e-01, 3.38801187e-02, 2.09618699e-01, 1.24338679e-01,\n",
              "       4.64435189e-02, 1.49054252e-01, 5.17425715e-01, 2.12342316e-01,\n",
              "       5.37346859e-01, 3.92008601e-01, 3.23501148e-01, 1.08592787e-02,\n",
              "       1.53131250e-01, 6.64396835e-02, 6.64742620e-02, 9.04493402e-02,\n",
              "       5.97501303e-02, 1.50732161e-01, 7.32852239e-02, 9.47929090e-02,\n",
              "       2.84835538e-02, 1.89842009e-01, 3.29877366e-01, 1.59836728e-01,\n",
              "       2.22405343e-01, 2.23917485e-01, 2.17184178e-01, 9.38958781e-02,\n",
              "       2.51203866e-01, 1.50018760e-02, 2.59842922e-01, 2.61843088e-01,\n",
              "       2.90769099e-01, 2.03836847e-01, 2.27191600e-01, 2.90216758e-01,\n",
              "       2.36075544e-01, 4.70758067e-01, 3.14897031e-02, 1.98635815e-01,\n",
              "       1.61213857e-01, 7.35527357e-02, 8.20444114e-02, 1.98511706e-01,\n",
              "       1.18743512e-01, 1.08814670e-01, 3.48128319e-01, 2.39098806e-01,\n",
              "       5.92914781e-03, 1.20139080e-01, 1.77444204e-01, 1.25227365e-01,\n",
              "       1.00783736e-01, 2.33070129e-01, 1.47498414e-01, 6.75393779e-04,\n",
              "       7.36014391e-02, 6.74100767e-02, 8.59166426e-02, 1.75601588e-01,\n",
              "       1.58994358e-01, 3.18803300e-01, 1.00242862e-01, 8.11579863e-03,\n",
              "       2.44943234e-02, 1.43894213e-01, 5.01280927e-01, 1.99965600e-02,\n",
              "       1.93345263e-01, 6.48278857e-01, 7.49955969e-03, 1.02100501e-01,\n",
              "       9.30911758e-02, 1.21228031e-01, 1.64138553e-01, 1.92592738e-01,\n",
              "       3.83918890e-01, 2.47032291e-01, 2.39948628e-01, 2.82078019e-02,\n",
              "       1.71537412e-01, 1.40539198e-16, 2.57997606e-01, 4.03113231e-01,\n",
              "       4.22113601e-01, 2.94775997e-01, 2.75151574e-03, 3.99383689e-01,\n",
              "       3.16957780e-01, 2.26986301e-01, 2.87425937e-01, 2.50358027e-01,\n",
              "       1.40691992e-02, 2.48364884e-01, 3.83285289e-01, 7.29862319e-02,\n",
              "       2.47066674e-01, 4.12195541e-01, 3.20853285e-02, 1.85123160e-02,\n",
              "       9.81289901e-02, 2.28634795e-01, 8.98430281e-02, 1.29707815e-01,\n",
              "       5.69016650e-01, 1.45485480e-01, 4.94892877e-01, 2.21396923e-01,\n",
              "       3.39216278e-01, 4.23444271e-01, 1.30123763e-01, 1.05185964e-01,\n",
              "       9.94034350e-02, 1.89092294e-01, 3.77065137e-02, 3.60030312e-01,\n",
              "       9.03037217e-02, 8.69331109e-03, 2.26533113e-01, 2.88656951e-02,\n",
              "       1.78813455e-01, 1.58516729e-01, 3.09868891e-02, 8.25166374e-17,\n",
              "       3.43425526e-02, 1.19173688e-01, 3.58048407e-02, 3.58048407e-02,\n",
              "       4.57945394e-01, 1.77093555e-01, 1.98657746e-01, 4.22014849e-01,\n",
              "       4.97027474e-01, 2.05872382e-01, 1.33136462e-01, 1.08664840e-01,\n",
              "       3.39556542e-17, 2.09271611e-02, 6.20433996e-02, 3.64589641e-02,\n",
              "       1.94649928e-01, 2.01365565e-02, 2.05144909e-01, 2.10650765e-01,\n",
              "       2.09202414e-01, 4.46684113e-03, 1.29575582e-01, 1.12609425e-01,\n",
              "       2.77860967e-01, 7.53504511e-02, 4.23904910e-02, 1.82999000e-01,\n",
              "       1.80939951e-01, 2.50229767e-01, 4.88699832e-01, 1.91434445e-01,\n",
              "       2.31070661e-02, 1.22628449e-02, 8.09544061e-02, 3.57581825e-02,\n",
              "       1.09272557e-01, 1.91867274e-01, 1.88111184e-01, 2.04777137e-01,\n",
              "       1.91309028e-01, 1.88768202e-01, 9.66522274e-02, 1.49436021e-01,\n",
              "       9.50750528e-02, 2.39186110e-01, 3.23223484e-02, 2.00157267e-01,\n",
              "       1.65863260e-01, 2.75722607e-02, 1.01094862e-01, 1.98517723e-02,\n",
              "       4.67998813e-02])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn9EjCGpFRCH"
      },
      "source": [
        "top_sentence_indices = (-salience_scores).argsort()[:num_sentences]\n",
        "top_sentence_indices.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLjP9KgbFUNi",
        "outputId": "dd32110e-b09e-45cc-9bc9-0369f21d4a2f"
      },
      "source": [
        "print('\\n'.join(np.array(sentences)[top_sentence_indices]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now that we have these two parameters of our neural network of our dense layer.\n",
            "So let's build a neural network with two inputs.\n",
            "Data.\n",
            "In the data set.\n",
            "There's only two weights in this neural network, very simple neural network.\n",
            "We compute the gradient that tells us which way is up.\n",
            "This is the how we actually compute the gradient.\n",
            "Why we compute the gradient of our loss with respect to each of the weights in our neural.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwy27oUUFVwC",
        "outputId": "806253a3-9212-4d68-c833-af0342cfe180"
      },
      "source": [
        "similarity_matrix = np.matmul(dt_matrix, dt_matrix.T)\n",
        "print(similarity_matrix.shape)\n",
        "np.round(similarity_matrix, 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(605, 605)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.   , 0.   , 0.   , ..., 0.   , 0.   , 0.   ],\n",
              "       [0.   , 1.   , 0.   , ..., 0.079, 0.   , 0.   ],\n",
              "       [0.   , 0.   , 1.   , ..., 0.   , 0.   , 0.   ],\n",
              "       ...,\n",
              "       [0.   , 0.079, 0.   , ..., 1.   , 0.   , 0.   ],\n",
              "       [0.   , 0.   , 0.   , ..., 0.   , 1.   , 0.415],\n",
              "       [0.   , 0.   , 0.   , ..., 0.   , 0.415, 1.   ]])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XKLLMxVFZFu",
        "outputId": "65c2e399-6436-4ea3-fafd-d76e4ea76715"
      },
      "source": [
        "import networkx\n",
        "\n",
        "similarity_graph = networkx.from_numpy_array(similarity_matrix)\n",
        "similarity_graph"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<networkx.classes.graph.Graph at 0x7f620e398950>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9M9Wz2lFdej",
        "outputId": "12c92e52-7328-4c0f-9e15-ae62dd574c15"
      },
      "source": [
        "scores = networkx.pagerank(similarity_graph)\n",
        "ranked_sentences = sorted(((score, index) for index, score \n",
        "                                            in scores.items()), \n",
        "                          reverse=True)\n",
        "ranked_sentences[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.004435419267154044, 351),\n",
              " (0.003762153644463771, 122),\n",
              " (0.003641559303693242, 302),\n",
              " (0.003606040367916521, 350),\n",
              " (0.0035976309788030366, 497),\n",
              " (0.0034834333310181133, 401),\n",
              " (0.0034736734931386294, 239),\n",
              " (0.003410276705119777, 410),\n",
              " (0.003332032693428844, 403),\n",
              " (0.00320217673050174, 360)]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62NO_yc0Ff5C"
      },
      "source": [
        "top_sentence_indices = [ranked_sentences[index][1] \n",
        "                        for index in range(3*num_sentences)]\n",
        "top_sentence_indices.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idmgKnTuFiyg",
        "outputId": "f4ab2cf5-e168-4251-9fad-02cad0dcda22"
      },
      "source": [
        "print('\\n\\n'.join(np.array(sentences)[top_sentence_indices]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So, let's start actually with the fundamental building block of deep learning and of every neural network that is just a single, neuron also known as a perceptron.\n",
            "\n",
            "We can actually, actually, this is not entirely correct, because one thing I forgot to mention is that we also have what's called a bias term in here, which allows you to shift your activation function left or right now, on the right hand side of this diagram, you can actually see this concept Illustrated or written out mathematically as a single equation.\n",
            "\n",
            "You might think this is easy with multiple lines, are curved lines, but you cannot only use a single straight line and that's what using a neural network with a linear activation function will be like that makes the problem really hard because no matter how deep the neural network is, you'll only be able to produce a single line decision boundary and you'll only able to separate your space with one line.\n",
            "\n",
            "We can illustrate this space actually, but this feature When we're dealing with a small dimensional data, like in this case, we only have two Dimensions but soon will start to talk about problems where we have thousands or millions or in some cases even billions of input of Weights in our neural network, and then drawing these types of plots becomes extremely challenging and not really possible anymore.\n",
            "\n",
            "Let's try and see how we can actually build up a dense layer like this.\n",
            "\n",
            "Now, let's take a look at what's called a single layered neural network.\n",
            "\n",
            "So let's build a neural network with two inputs.\n",
            "\n",
            "We also need to use a different loss here because our outputs are no longer 01, but they Any real number third this, the grade that you're going to get on the final class.\n",
            "\n",
            "How can we use our loss function, to train the weights of our neural networks such that it can actually learn that problem.\n",
            "\n",
            "Well, what we want to do is actually find the weights of the neural network that will minimize the loss of our data set.\n",
            "\n",
            "Now remember that W capital W is simply a collection of all of the weights in our neural network, not just from one layer, but from every single layer, so that's w0 from the zeroth layer to the first layer to the second layer all concatenate into one.\n",
            "\n",
            "There's only two weights in this neural network, very simple neural network.\n",
            "\n",
            "Okay, if we compute the gradient of our laws, with respect to our weights, that's the derivative our gradient for loss with respect to the weights.\n",
            "\n",
            "We compute the gradient, and we take a small step of our weights in the direction of that.\n",
            "\n",
            "This is the how we actually compute the gradient.\n",
            "\n",
            "So let's talk about this process, which is actually extremely important, in training, neural networks.\n",
            "\n",
            "Let's start with a very simple neural network.\n",
            "\n",
            "It only has one input, one hidden neuron, and one output Computing, the gradient of our loss, J of w with respect to one of the weights in this case.\n",
            "\n",
            "Now, you can see starting from our loss all the way through W2 and then recursively applying this chain rule again to get to W 1 and this allows us to see both the gradient at both W 2 and W 1.\n",
            "\n",
            "Why we compute the gradient of our loss with respect to each of the weights in our neural.\n",
            "\n",
            "Let's just pick a single point compute, the gradient of that single point with respect.\n",
            "\n",
            "This is very easy to compute because only using one data point.\n",
            "\n",
            "It also means that we can trust our gradient more than in stochastic gradient descent, so that we can actually increase our Learning grade a bit more as well.\n",
            "\n",
            "We want to learn a model that accurately describes our test data, not the training data, even though we're optimizing this model based on the training data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDrpSowUvaKW"
      },
      "source": [
        "So, let's start actually with the fundamental building block of deep learning and of every neural network that is just a single, neuron also known as a perceptron.\n",
        "\n",
        "We can actually, actually, this is not entirely correct, because one thing I forgot to mention is that we also have what's called a bias term in here, which allows you to shift your activation function left or right now, on the right hand side of this diagram, you can actually see this concept Illustrated or written out mathematically as a single equation.\n",
        "\n",
        "You might think this is easy with multiple lines, are curved lines, but you cannot only use a single straight line and that's what using a neural network with a linear activation function will be like that makes the problem really hard because no matter how deep the neural network is, you'll only be able to produce a single line decision boundary and you'll only able to separate your space with one line.\n",
        "\n",
        "We can illustrate this space actually, but this feature When we're dealing with a small dimensional data, like in this case, we only have two Dimensions but soon will start to talk about problems where we have thousands or millions or in some cases even billions of input of Weights in our neural network, and then drawing these types of plots becomes extremely challenging and not really possible anymore.\n",
        "\n",
        "Let's try and see how we can actually build up a dense layer like this.\n",
        "\n",
        "Now, let's take a look at what's called a single layered neural network.\n",
        "\n",
        "So let's build a neural network with two inputs.\n",
        "\n",
        "We also need to use a different loss here because our outputs are no longer 01, but they Any real number third this, the grade that you're going to get on the final class.\n",
        "\n",
        "How can we use our loss function, to train the weights of our neural networks such that it can actually learn that problem.\n",
        "\n",
        "Well, what we want to do is actually find the weights of the neural network that will minimize the loss of our data set.\n",
        "\n",
        "Now remember that W capital W is simply a collection of all of the weights in our neural network, not just from one layer, but from every single layer, so that's w0 from the zeroth layer to the first layer to the second layer all concatenate into one.\n",
        "\n",
        "There's only two weights in this neural network, very simple neural network.\n",
        "\n",
        "Okay, if we compute the gradient of our laws, with respect to our weights, that's the derivative our gradient for loss with respect to the weights.\n",
        "\n",
        "We compute the gradient, and we take a small step of our weights in the direction of that.\n",
        "\n",
        "This is the how we actually compute the gradient.\n",
        "\n",
        "So let's talk about this process, which is actually extremely important, in training, neural networks.\n",
        "\n",
        "Let's start with a very simple neural network.\n",
        "\n",
        "It only has one input, one hidden neuron, and one output Computing, the gradient of our loss, J of w with respect to one of the weights in this case.\n",
        "\n",
        "Now, you can see starting from our loss all the way through W2 and then recursively applying this chain rule again to get to W 1 and this allows us to see both the gradient at both W 2 and W 1.\n",
        "\n",
        "Why we compute the gradient of our loss with respect to each of the weights in our neural.\n",
        "\n",
        "Let's just pick a single point compute, the gradient of that single point with respect.\n",
        "\n",
        "This is very easy to compute because only using one data point.\n",
        "\n",
        "It also means that we can trust our gradient more than in stochastic gradient descent, so that we can actually increase our Learning grade a bit more as well.\n",
        "\n",
        "We want to learn a model that accurately describes our test data, not the training data, even though we're optimizing this model based on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZi3OnN6uG-H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}